"""
å…¨æ–°çš„RAGASè¯„ä¼°å™¨ - åŸºäºçœŸå®æ¨ç†æ•°æ®
ä¸“é—¨ä¸ºACRACåŒ»å­¦æ¨èç³»ç»Ÿè®¾è®¡
"""
import os
import json
import logging
import asyncio
from typing import Dict, List, Any, Optional
import numpy as np
import pandas as pd
from pathlib import Path

try:
    from ragas.dataset_schema import SingleTurnSample
    from ragas.metrics import Faithfulness, ContextPrecision, ContextRecall
    from langchain_openai import ChatOpenAI, OpenAIEmbeddings
    RAGAS_AVAILABLE = True
except ImportError as e:
    logging.warning(f"RAGASç›¸å…³ä¾èµ–æœªå®‰è£…: {e}")
    RAGAS_AVAILABLE = False

logger = logging.getLogger(__name__)

class ACRACRAGASEvaluator:
    """ACRACä¸“ç”¨RAGASè¯„ä¼°å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        if not RAGAS_AVAILABLE:
            raise ImportError("RAGASç›¸å…³ä¾èµ–æœªå®‰è£…")
        
        # è®¾ç½®äº‹ä»¶å¾ªç¯å…¼å®¹æ€§
        self._setup_event_loop()
        
        # åŠ è½½é…ç½®
        self._load_config()
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._init_models()
        
        # åˆå§‹åŒ–æŒ‡æ ‡ï¼ˆåªä½¿ç”¨ç¨³å®šçš„æŒ‡æ ‡ï¼‰
        self._init_metrics()
        
        logger.info(f"ACRAC RAGASè¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ - LLM: {self.llm_model}")

    def _setup_event_loop(self):
        """è®¾ç½®äº‹ä»¶å¾ªç¯å…¼å®¹æ€§"""
        os.environ['NEST_ASYNCIO_DISABLE'] = '1'
        os.environ['UVLOOP_DISABLE'] = '1'
        try:
            asyncio.set_event_loop_policy(asyncio.DefaultEventLoopPolicy())
        except Exception as e:
            logger.warning(f"äº‹ä»¶å¾ªç¯è®¾ç½®å¤±è´¥: {e}")

    def _load_config(self):
        """åŠ è½½é…ç½®"""
        # ä»ç¯å¢ƒå˜é‡åŠ è½½APIé…ç½®
        self.api_key = os.getenv("SILICONFLOW_API_KEY") or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("æœªæ‰¾åˆ°APIå¯†é’¥ï¼Œè¯·è®¾ç½® SILICONFLOW_API_KEY")
        
        self.base_url = os.getenv("SILICONFLOW_BASE_URL", "https://api.siliconflow.cn/v1")
        self.llm_model = os.getenv("SILICONFLOW_LLM_MODEL", "Qwen/Qwen2.5-32B-Instruct")
        self.embedding_model = os.getenv("SILICONFLOW_EMBEDDING_MODEL", "BAAI/bge-m3")

    def _init_models(self):
        """åˆå§‹åŒ–LLMå’ŒåµŒå…¥æ¨¡å‹"""
        self.llm = ChatOpenAI(
            model=self.llm_model,
            api_key=self.api_key,
            base_url=self.base_url,
            temperature=0.1,
            timeout=60,
            max_retries=2
        )
        
        self.embeddings = OpenAIEmbeddings(
            model=self.embedding_model,
            api_key=self.api_key,
            base_url=self.base_url,
            timeout=60,
            max_retries=2
        )
        # ç»Ÿä¸€å¯¹å¤–å¯è§çš„æ¨¡å‹åå­—æ®µï¼ˆä¾›ä¸Šå±‚è®°å½•/å±•ç¤ºï¼‰
        self.llm_model_name = self.llm_model
        self.embedding_model_name = self.embedding_model

    def _init_metrics(self):
        """åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡ï¼ˆåªä½¿ç”¨ç¨³å®šçš„æŒ‡æ ‡ï¼‰"""
        self.faithfulness = Faithfulness(llm=self.llm)
        self.context_precision = ContextPrecision(llm=self.llm)
        self.context_recall = ContextRecall(llm=self.llm)
        
        # æš‚æ—¶ä¸ä½¿ç”¨answer_relevancyï¼Œå› ä¸ºå®ƒæœ‰LLMè¾“å‡ºè§£æé—®é¢˜
        logger.info("åˆå§‹åŒ–3ä¸ªç¨³å®šæŒ‡æ ‡: faithfulness, context_precision, context_recall")

    def create_sample(self, data: Dict[str, Any]) -> SingleTurnSample:
        """åˆ›å»ºRAGASæ ·æœ¬å¯¹è±¡"""
        question = str(data.get('question', ''))
        answer = str(data.get('answer', ''))
        contexts = data.get('contexts', [])
        ground_truth = str(data.get('ground_truth', ''))
        
        # ç¡®ä¿contextsæ˜¯å­—ç¬¦ä¸²åˆ—è¡¨
        if isinstance(contexts, str):
            contexts = [contexts]
        elif not isinstance(contexts, list):
            contexts = [str(contexts)]
        
        # è¿‡æ»¤ç©ºçš„ä¸Šä¸‹æ–‡
        contexts = [ctx for ctx in contexts if ctx and str(ctx).strip()]
        if not contexts:
            contexts = ["ç›¸å…³åŒ»å­¦çŸ¥è¯†"]  # é»˜è®¤ä¸Šä¸‹æ–‡
        
        return SingleTurnSample(
            user_input=question,
            response=answer,
            retrieved_contexts=contexts,
            reference=ground_truth
        )

    async def evaluate_sample_async(self, data: Dict[str, Any]) -> Dict[str, float]:
        """å¼‚æ­¥è¯„ä¼°å•ä¸ªæ ·æœ¬"""
        try:
            sample = self.create_sample(data)
            logger.info(f"è¯„ä¼°æ ·æœ¬: {sample.user_input[:50]}...")
            
            results = {}
            
            # è¯„ä¼°faithfulness
            try:
                score = await self.faithfulness.single_turn_ascore(sample)
                results['faithfulness'] = float(score) if not pd.isna(score) else 0.0
                logger.info(f"faithfulness: {results['faithfulness']:.4f}")
            except Exception as e:
                logger.error(f"faithfulnessè¯„ä¼°å¤±è´¥: {e}")
                results['faithfulness'] = 0.0
            
            # è¯„ä¼°context_precision
            try:
                score = await self.context_precision.single_turn_ascore(sample)
                results['context_precision'] = float(score) if not pd.isna(score) else 0.0
                logger.info(f"context_precision: {results['context_precision']:.4f}")
            except Exception as e:
                logger.error(f"context_precisionè¯„ä¼°å¤±è´¥: {e}")
                results['context_precision'] = 0.0
            
            # è¯„ä¼°context_recall
            try:
                score = await self.context_recall.single_turn_ascore(sample)
                results['context_recall'] = float(score) if not pd.isna(score) else 0.0
                logger.info(f"context_recall: {results['context_recall']:.4f}")
            except Exception as e:
                logger.error(f"context_recallè¯„ä¼°å¤±è´¥: {e}")
                results['context_recall'] = 0.0
            
            # æš‚æ—¶å°†answer_relevancyè®¾ä¸º0ï¼Œé¿å…è§£æé—®é¢˜
            results['answer_relevancy'] = 0.0
            
            return results
            
        except Exception as e:
            logger.error(f"æ ·æœ¬è¯„ä¼°å¤±è´¥: {e}")
            return {
                'faithfulness': 0.0,
                'answer_relevancy': 0.0,
                'context_precision': 0.0,
                'context_recall': 0.0
            }

    def evaluate_sample(self, data: Dict[str, Any]) -> Dict[str, float]:
        """åŒæ­¥è¯„ä¼°å•ä¸ªæ ·æœ¬"""
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                return loop.run_until_complete(self.evaluate_sample_async(data))
            finally:
                loop.close()
        except Exception as e:
            logger.error(f"åŒæ­¥è¯„ä¼°å¤±è´¥: {e}")
            return {
                'faithfulness': 0.0,
                'answer_relevancy': 0.0,
                'context_precision': 0.0,
                'context_recall': 0.0
            }

    def evaluate_batch(self, data_list: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ‰¹é‡è¯„ä¼°"""
        if not data_list:
            return {
                'avg_scores': {'faithfulness': 0.0, 'answer_relevancy': 0.0, 'context_precision': 0.0, 'context_recall': 0.0},
                'individual_scores': [],
                'total_samples': 0
            }
        
        individual_scores = []
        
        for i, data in enumerate(data_list):
            logger.info(f"è¯„ä¼°æ ·æœ¬ {i+1}/{len(data_list)}")
            scores = self.evaluate_sample(data)
            individual_scores.append(scores)
        
        # è®¡ç®—å¹³å‡åˆ†
        avg_scores = {}
        for metric in ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']:
            scores = [s[metric] for s in individual_scores]
            avg_scores[metric] = np.mean(scores) if scores else 0.0
        
        return {
            'avg_scores': avg_scores,
            'individual_scores': individual_scores,
            'total_samples': len(data_list)
        }

    def load_real_data(self, file_path: str) -> List[Dict[str, Any]]:
        """åŠ è½½çœŸå®æ¨ç†æ•°æ®"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            logger.info(f"åŠ è½½äº† {len(data)} æ¡çœŸå®æ¨ç†æ•°æ®")
            return data
            
        except Exception as e:
            logger.error(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return []

    def save_results(self, results: Dict[str, Any], output_path: str):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        try:
            output_file = Path(output_path)
            output_file.parent.mkdir(parents=True, exist_ok=True)
            
            # æ·»åŠ æ—¶é—´æˆ³å’Œå…ƒæ•°æ®
            results['evaluation_metadata'] = {
                'timestamp': pd.Timestamp.now().isoformat(),
                'evaluator_version': 'ACRAC_RAGAS_V2',
                'llm_model': self.llm_model,
                'embedding_model': self.embedding_model
            }
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")


if __name__ == "__main__":
    # æµ‹è¯•æ–°çš„è¯„ä¼°å™¨
    print("ğŸš€ æµ‹è¯•ACRAC RAGASè¯„ä¼°å™¨")
    
    # è®¾ç½®è¯¦ç»†æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    env_file = Path(__file__).parent.parent.parent / '.env'
    if env_file.exists():
        with open(env_file, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key] = value
    
    try:
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = ACRACRAGASEvaluator()
        
        # åŠ è½½çœŸå®æ•°æ®
        data_file = "correct_ragas_data_20250924_021143.json"
        real_data = evaluator.load_real_data(data_file)
        
        if real_data:
            print(f"ğŸ“Š å¼€å§‹è¯„ä¼° {len(real_data)} æ¡çœŸå®æ•°æ®")
            
            # è¿è¡Œæ‰¹é‡è¯„ä¼°
            results = evaluator.evaluate_batch(real_data)
            
            # æ˜¾ç¤ºç»“æœ
            print(f"\nâœ… è¯„ä¼°å®Œæˆï¼")
            print(f"å¹³å‡è¯„åˆ†:")
            for metric, score in results['avg_scores'].items():
                status = "âœ…" if score > 0 else "âš ï¸"
                print(f"  {status} {metric}: {score:.4f}")
            
            # ä¿å­˜ç»“æœ
            output_file = f"acrac_ragas_evaluation_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.json"
            evaluator.save_results(results, output_file)
            
            print(f"\nğŸ¯ è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        else:
            print("âŒ æœªæ‰¾åˆ°çœŸå®æ•°æ®")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        logger.error(f"è¯¦ç»†é”™è¯¯: {e}", exc_info=True)