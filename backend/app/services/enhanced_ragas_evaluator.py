#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆRAGASè¯„ä¼°å™¨ - æ¨¡å—åŒ–é‡æ„ç‰ˆæœ¬
ä¸“é—¨å¤„ç†ä¸­æ–‡åŒ»å­¦å†…å®¹ï¼Œæä¾›å¯é…ç½®çš„æ¨¡å‹ç®¡ç†å’Œè¯„æµ‹æŒ‡æ ‡
"""
import os
import json
import logging
import asyncio
from typing import Dict, List, Any, Optional, Union
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime

try:
    from ragas.dataset_schema import SingleTurnSample
    from ragas.metrics import Faithfulness, ContextPrecision, ContextRecall
    from langchain_openai import ChatOpenAI, OpenAIEmbeddings
    from langchain.schema import HumanMessage
    RAGAS_AVAILABLE = True
except ImportError as e:
    logging.warning(f"RAGASç›¸å…³ä¾èµ–æœªå®‰è£…: {e}")
    RAGAS_AVAILABLE = False

logger = logging.getLogger(__name__)


class EvaluationStatus(Enum):
    """è¯„æµ‹çŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"


@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    llm_model: str = "Qwen/Qwen2.5-32B-Instruct"
    embedding_model: str = "BAAI/bge-m3"
    api_key: str = ""
    base_url: str = "https://api.siliconflow.cn/v1"
    temperature: float = 0.1
    timeout: int = 60
    max_retries: int = 2


@dataclass
class EvaluationConfig:
    """è¯„æµ‹é…ç½®"""
    enable_faithfulness: bool = True
    enable_context_precision: bool = True
    enable_context_recall: bool = True
    enable_answer_relevancy: bool = True
    use_enhanced_methods: bool = True
    chinese_optimization: bool = True
    medical_domain: bool = True


@dataclass
class EvaluationResult:
    """è¯„æµ‹ç»“æœ"""
    sample_id: str
    metrics: Dict[str, float]
    enhanced_metrics: Dict[str, float]
    llm_parsing_success: bool
    fallback_method_used: Optional[str]
    medical_term_analysis: Dict[str, Any]
    chinese_processing_info: Dict[str, Any]
    process_data: Dict[str, Any]
    status: EvaluationStatus
    error_message: Optional[str]
    created_at: str


class BaseEvaluator(ABC):
    """è¯„æµ‹å™¨åŸºç±»"""
    
    @abstractmethod
    async def evaluate_sample(self, sample: SingleTurnSample) -> Dict[str, float]:
        """è¯„æµ‹å•ä¸ªæ ·æœ¬"""
        pass
    
    @abstractmethod
    async def evaluate_batch(self, samples: List[SingleTurnSample]) -> List[Dict[str, float]]:
        """æ‰¹é‡è¯„æµ‹æ ·æœ¬"""
        pass


class EnhancedRAGASEvaluator(BaseEvaluator):
    """å¢å¼ºç‰ˆRAGASè¯„ä¼°å™¨ï¼Œä¸“é—¨å¤„ç†ä¸­æ–‡åŒ»å­¦å†…å®¹"""
    
    def __init__(self, model_config: Optional[ModelConfig] = None, 
                 evaluation_config: Optional[EvaluationConfig] = None):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        if not RAGAS_AVAILABLE:
            raise ImportError("RAGASç›¸å…³ä¾èµ–æœªå®‰è£…")
        
        # è®¾ç½®äº‹ä»¶å¾ªç¯å…¼å®¹æ€§
        self._setup_event_loop()
        
        # åŠ è½½é…ç½®
        self.model_config = model_config or self._load_default_model_config()
        self.evaluation_config = evaluation_config or EvaluationConfig()
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._init_models()
        
        # åˆå§‹åŒ–è¯„æµ‹æŒ‡æ ‡
        self._init_metrics()
        
        logger.info(f"å¢å¼ºç‰ˆRAGASè¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ - LLM: {self.model_config.llm_model}")

    def _setup_event_loop(self):
        """è®¾ç½®äº‹ä»¶å¾ªç¯å…¼å®¹æ€§"""
        try:
            asyncio.set_event_loop_policy(asyncio.DefaultEventLoopPolicy())
        except Exception as e:
            logger.warning(f"äº‹ä»¶å¾ªç¯è®¾ç½®å¤±è´¥: {e}")

    def _load_default_model_config(self) -> ModelConfig:
        """åŠ è½½é»˜è®¤æ¨¡å‹é…ç½®"""
        api_key = os.getenv("SILICONFLOW_API_KEY") or os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("æœªæ‰¾åˆ°APIå¯†é’¥ï¼Œè¯·è®¾ç½® SILICONFLOW_API_KEY")
        
        return ModelConfig(
            api_key=api_key,
            base_url=os.getenv("SILICONFLOW_BASE_URL", "https://api.siliconflow.cn/v1"),
            llm_model=os.getenv("SILICONFLOW_LLM_MODEL", "Qwen/Qwen2.5-32B-Instruct"),
            embedding_model=os.getenv("SILICONFLOW_EMBEDDING_MODEL", "BAAI/bge-m3")
        )

    def _init_models(self):
        """åˆå§‹åŒ–LLMå’ŒåµŒå…¥æ¨¡å‹"""
        self.llm = ChatOpenAI(
            model=self.model_config.llm_model,
            api_key=self.model_config.api_key,
            base_url=self.model_config.base_url,
            temperature=self.model_config.temperature,
            timeout=self.model_config.timeout,
            max_retries=self.model_config.max_retries
        )
        
        self.embeddings = OpenAIEmbeddings(
            model=self.model_config.embedding_model,
            api_key=self.model_config.api_key,
            base_url=self.model_config.base_url,
            timeout=self.model_config.timeout,
            max_retries=self.model_config.max_retries
        )

    def _init_metrics(self):
        """åˆå§‹åŒ–è¯„æµ‹æŒ‡æ ‡"""
        self.metrics = {}
        
        if self.evaluation_config.enable_faithfulness:
            self.metrics['faithfulness'] = Faithfulness()
        
        if self.evaluation_config.enable_context_precision:
            self.metrics['context_precision'] = ContextPrecision()
        
        if self.evaluation_config.enable_context_recall:
            self.metrics['context_recall'] = ContextRecall()

    def create_sample(self, data: Dict[str, Any]) -> SingleTurnSample:
        """åˆ›å»ºRAGASæ ·æœ¬å¯¹è±¡"""
        question = str(data.get('question', ''))
        answer = str(data.get('answer', ''))
        contexts = data.get('contexts', [])
        ground_truth = str(data.get('ground_truth', ''))
        
        # ç¡®ä¿contextsæ˜¯å­—ç¬¦ä¸²åˆ—è¡¨
        if isinstance(contexts, str):
            contexts = [contexts]
        elif not isinstance(contexts, list):
            contexts = [str(contexts)]
        
        # è¿‡æ»¤ç©ºçš„ä¸Šä¸‹æ–‡
        contexts = [ctx for ctx in contexts if ctx and str(ctx).strip()]
        if not contexts:
            contexts = ["ç›¸å…³åŒ»å­¦çŸ¥è¯†"]  # é»˜è®¤ä¸Šä¸‹æ–‡
        
        return SingleTurnSample(
            user_input=question,
            response=answer,
            retrieved_contexts=contexts,
            reference=ground_truth
        )

    async def _evaluate_faithfulness_enhanced(self, sample: SingleTurnSample) -> Dict[str, Any]:
        """å¢å¼ºç‰ˆå¿ å®åº¦è¯„ä¼°"""
        try:
            # ä½¿ç”¨ç®€åŒ–çš„å¿ å®åº¦è¯„ä¼°
            contexts_text = "\n".join(sample.retrieved_contexts)
            prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹ç­”æ¡ˆæ˜¯å¦å¿ å®äºç»™å®šçš„ä¸Šä¸‹æ–‡ã€‚

ä¸Šä¸‹æ–‡ï¼š
{contexts_text}

é—®é¢˜ï¼š{sample.user_input}

ç­”æ¡ˆï¼š{sample.response}

è¯·åˆ¤æ–­ç­”æ¡ˆä¸­çš„ä¿¡æ¯æ˜¯å¦éƒ½èƒ½ä»ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°æ”¯æŒã€‚
è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
  "faithfulness_score": 0.8,
  "explanation": "è¯„ä¼°è¯´æ˜"
}}

è¯„åˆ†æ ‡å‡†ï¼š
- 1.0: ç­”æ¡ˆå®Œå…¨åŸºäºä¸Šä¸‹æ–‡
- 0.8: ç­”æ¡ˆå¤§éƒ¨åˆ†åŸºäºä¸Šä¸‹æ–‡
- 0.6: ç­”æ¡ˆéƒ¨åˆ†åŸºäºä¸Šä¸‹æ–‡
- 0.4: ç­”æ¡ˆå°‘éƒ¨åˆ†åŸºäºä¸Šä¸‹æ–‡
- 0.2: ç­”æ¡ˆåŸºæœ¬ä¸åŸºäºä¸Šä¸‹æ–‡
- 0.0: ç­”æ¡ˆä¸ä¸Šä¸‹æ–‡æ— å…³
"""
            
            response = self.llm.invoke([HumanMessage(content=prompt)])
            
            try:
                result = json.loads(response.content)
                score = float(result.get('faithfulness_score', 0.0))
                explanation = result.get('explanation', '')
                logger.info(f"å¢å¼ºç‰ˆfaithfulness: {score:.4f}")
                return {
                    'score': score,
                    'explanation': explanation,
                    'llm_parsing_success': True,
                    'fallback_method': None
                }
            except:
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–åˆ†æ•°
                content = response.content.lower()
                if '1.0' in content or 'å®Œå…¨' in content:
                    score = 1.0
                elif '0.8' in content or 'å¤§éƒ¨åˆ†' in content:
                    score = 0.8
                elif '0.6' in content or 'éƒ¨åˆ†' in content:
                    score = 0.6
                elif '0.4' in content or 'å°‘éƒ¨åˆ†' in content:
                    score = 0.4
                elif '0.2' in content or 'åŸºæœ¬ä¸' in content:
                    score = 0.2
                else:
                    score = 0.0
                
                return {
                    'score': score,
                    'explanation': response.content,
                    'llm_parsing_success': False,
                    'fallback_method': 'text_parsing'
                }
                    
        except Exception as e:
            logger.error(f"å¢å¼ºç‰ˆfaithfulnessè¯„ä¼°å¤±è´¥: {e}")
            return {
                'score': 0.0,
                'explanation': f"è¯„ä¼°å¤±è´¥: {str(e)}",
                'llm_parsing_success': False,
                'fallback_method': 'error_fallback'
            }

    async def _evaluate_context_precision_enhanced(self, sample: SingleTurnSample) -> Dict[str, Any]:
        """å¢å¼ºç‰ˆä¸Šä¸‹æ–‡ç²¾ç¡®åº¦è¯„ä¼°"""
        try:
            contexts_numbered = "\n".join([f"{i+1}. {ctx}" for i, ctx in enumerate(sample.retrieved_contexts)])
            prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹ä¸Šä¸‹æ–‡å¯¹å›ç­”é—®é¢˜çš„ç²¾ç¡®åº¦ã€‚

é—®é¢˜ï¼š{sample.user_input}

ä¸Šä¸‹æ–‡ï¼š
{contexts_numbered}

ç­”æ¡ˆï¼š{sample.response}

è¯·åˆ¤æ–­æ¯ä¸ªä¸Šä¸‹æ–‡æ˜¯å¦å¯¹å›ç­”é—®é¢˜æœ‰ç”¨ã€‚
è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
  "precision_score": 0.8,
  "useful_contexts": [1, 2],
  "explanation": "è¯„ä¼°è¯´æ˜"
}}

è¯„åˆ† = æœ‰ç”¨çš„ä¸Šä¸‹æ–‡æ•°é‡ / æ€»ä¸Šä¸‹æ–‡æ•°é‡
"""
            
            response = self.llm.invoke([HumanMessage(content=prompt)])
            
            try:
                result = json.loads(response.content)
                score = float(result.get('precision_score', 0.0))
                useful_contexts = result.get('useful_contexts', [])
                explanation = result.get('explanation', '')
                logger.info(f"å¢å¼ºç‰ˆcontext_precision: {score:.4f}")
                return {
                    'score': score,
                    'useful_contexts': useful_contexts,
                    'explanation': explanation,
                    'llm_parsing_success': True,
                    'fallback_method': None
                }
            except:
                # ç®€å•çš„å¯å‘å¼è¯„ä¼°
                total_contexts = len(sample.retrieved_contexts)
                if total_contexts == 0:
                    return {
                        'score': 0.0,
                        'useful_contexts': [],
                        'explanation': 'æ— ä¸Šä¸‹æ–‡',
                        'llm_parsing_success': False,
                        'fallback_method': 'heuristic'
                    }
                
                # æ£€æŸ¥ä¸Šä¸‹æ–‡ä¸é—®é¢˜çš„ç›¸å…³æ€§
                question_lower = sample.user_input.lower()
                useful_count = 0
                
                for ctx in sample.retrieved_contexts:
                    ctx_lower = ctx.lower()
                    # ç®€å•çš„å…³é”®è¯åŒ¹é…
                    if any(word in ctx_lower for word in question_lower.split() if len(word) > 1):
                        useful_count += 1
                
                score = useful_count / total_contexts
                logger.info(f"å¢å¼ºç‰ˆcontext_precision (å¯å‘å¼): {score:.4f}")
                return {
                    'score': score,
                    'useful_contexts': list(range(1, useful_count + 1)),
                    'explanation': f'å¯å‘å¼è¯„ä¼°: {useful_count}/{total_contexts} ä¸ªä¸Šä¸‹æ–‡æœ‰ç”¨',
                    'llm_parsing_success': False,
                    'fallback_method': 'heuristic'
                }
                
        except Exception as e:
            logger.error(f"å¢å¼ºç‰ˆcontext_precisionè¯„ä¼°å¤±è´¥: {e}")
            return {
                'score': 0.0,
                'useful_contexts': [],
                'explanation': f"è¯„ä¼°å¤±è´¥: {str(e)}",
                'llm_parsing_success': False,
                'fallback_method': 'error_fallback'
            }

    async def _evaluate_context_recall_enhanced(self, sample: SingleTurnSample) -> Dict[str, Any]:
        """å¢å¼ºç‰ˆä¸Šä¸‹æ–‡å¬å›ç‡è¯„ä¼°"""
        try:
            if not sample.reference or sample.reference.strip() == "":
                # å¦‚æœæ²¡æœ‰æ ‡å‡†ç­”æ¡ˆï¼Œä½¿ç”¨ç­”æ¡ˆæœ¬èº«ä½œä¸ºå‚è€ƒ
                reference = sample.response
            else:
                reference = sample.reference
            
            contexts_text = "\n".join(sample.retrieved_contexts)
            prompt = f"""è¯·è¯„ä¼°ä¸Šä¸‹æ–‡æ˜¯å¦åŒ…å«äº†å›ç­”é—®é¢˜æ‰€éœ€çš„ä¿¡æ¯ã€‚

é—®é¢˜ï¼š{sample.user_input}

æ ‡å‡†ç­”æ¡ˆ/å‚è€ƒç­”æ¡ˆï¼š{reference}

ä¸Šä¸‹æ–‡ï¼š
{contexts_text}

è¯·åˆ¤æ–­ä¸Šä¸‹æ–‡æ˜¯å¦åŒ…å«äº†å›ç­”é—®é¢˜æ‰€éœ€çš„å…³é”®ä¿¡æ¯ã€‚
è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
  "recall_score": 0.8,
  "explanation": "è¯„ä¼°è¯´æ˜"
}}

è¯„åˆ†æ ‡å‡†ï¼š
- 1.0: ä¸Šä¸‹æ–‡åŒ…å«æ‰€æœ‰å¿…è¦ä¿¡æ¯
- 0.8: ä¸Šä¸‹æ–‡åŒ…å«å¤§éƒ¨åˆ†å¿…è¦ä¿¡æ¯
- 0.6: ä¸Šä¸‹æ–‡åŒ…å«éƒ¨åˆ†å¿…è¦ä¿¡æ¯
- 0.4: ä¸Šä¸‹æ–‡åŒ…å«å°‘é‡å¿…è¦ä¿¡æ¯
- 0.2: ä¸Šä¸‹æ–‡åŒ…å«å¾ˆå°‘å¿…è¦ä¿¡æ¯
- 0.0: ä¸Šä¸‹æ–‡ä¸åŒ…å«å¿…è¦ä¿¡æ¯
"""
            
            response = self.llm.invoke([HumanMessage(content=prompt)])
            
            try:
                result = json.loads(response.content)
                score = float(result.get('recall_score', 0.0))
                explanation = result.get('explanation', '')
                # ç»“åˆGTå‡ºç°ä¸å¦è¿›è¡Œä¿å®ˆæ ¡å‡†
                if isinstance(reference, str) and reference.strip():
                    ref = reference.strip()
                    ctx_text = contexts_text
                    if ref in ctx_text:
                        score = max(score, 0.8)
                    else:
                        score = min(score, 0.6)
                logger.info(f"å¢å¼ºç‰ˆcontext_recall: {score:.4f}")
                return {
                    'score': score,
                    'explanation': explanation,
                    'llm_parsing_success': True,
                    'fallback_method': None
                }
            except:
                # å¯å‘å¼è¯„ä¼°
                if len(sample.retrieved_contexts) > 0:
                    # è‹¥GTå­˜åœ¨ä¸”æœªå‡ºç°åœ¨ä»»ä½•ä¸Šä¸‹æ–‡ä¸­ï¼Œåˆ™ç»™0.6ï¼›å¦åˆ™0.8
                    gt = (reference or "").strip() if isinstance(reference, str) else ""
                    if gt and all((gt not in (c or "")) for c in sample.retrieved_contexts):
                        score = 0.6
                    else:
                        score = 0.8
                else:
                    score = 0.0
                logger.info(f"å¢å¼ºç‰ˆcontext_recall (å¯å‘å¼): {score:.4f}")
                return {
                    'score': score,
                    'explanation': f'å¯å‘å¼è¯„ä¼°: {"æœ‰ä¸Šä¸‹æ–‡" if len(sample.retrieved_contexts) > 0 else "æ— ä¸Šä¸‹æ–‡"}',
                    'llm_parsing_success': False,
                    'fallback_method': 'heuristic'
                }
                
        except Exception as e:
            logger.error(f"å¢å¼ºç‰ˆcontext_recallè¯„ä¼°å¤±è´¥: {e}")
            return {
                'score': 0.0,
                'explanation': f"è¯„ä¼°å¤±è´¥: {str(e)}",
                'llm_parsing_success': False,
                'fallback_method': 'error_fallback'
            }

    async def _evaluate_answer_relevancy_enhanced(self, sample: SingleTurnSample) -> Dict[str, Any]:
        """å¢å¼ºç‰ˆç­”æ¡ˆç›¸å…³æ€§è¯„ä¼°"""
        try:
            prompt = f"""è¯·è¯„ä¼°ç­”æ¡ˆä¸é—®é¢˜çš„ç›¸å…³æ€§ã€‚

é—®é¢˜ï¼š{sample.user_input}

ç­”æ¡ˆï¼š{sample.response}

è¯·åˆ¤æ–­ç­”æ¡ˆæ˜¯å¦ç›´æ¥å›ç­”äº†é—®é¢˜ã€‚
è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
  "relevancy_score": 0.8,
  "explanation": "è¯„ä¼°è¯´æ˜"
}}

è¯„åˆ†æ ‡å‡†ï¼š
- 1.0: ç­”æ¡ˆå®Œå…¨å›ç­”äº†é—®é¢˜
- 0.8: ç­”æ¡ˆå¤§éƒ¨åˆ†å›ç­”äº†é—®é¢˜
- 0.6: ç­”æ¡ˆéƒ¨åˆ†å›ç­”äº†é—®é¢˜
- 0.4: ç­”æ¡ˆå°‘éƒ¨åˆ†å›ç­”äº†é—®é¢˜
- 0.2: ç­”æ¡ˆåŸºæœ¬æ²¡æœ‰å›ç­”é—®é¢˜
- 0.0: ç­”æ¡ˆä¸é—®é¢˜æ— å…³
"""
            
            response = self.llm.invoke([HumanMessage(content=prompt)])
            
            try:
                result = json.loads(response.content)
                score = float(result.get('relevancy_score', 0.0))
                explanation = result.get('explanation', '')
                # è‹¥å­˜åœ¨æ ‡å‡†ç­”æ¡ˆï¼Œä¼˜å…ˆè¿›è¡Œä¸€è‡´æ€§æ ¡å‡†
                if sample.reference and isinstance(sample.reference, str):
                    ref = sample.reference.strip()
                    ans = (sample.response or "").strip()
                    if ref:
                        if ref in ans:
                            score = max(score, 0.8)  # å‘½ä¸­GTè‡³å°‘0.8
                        else:
                            score = min(score, 0.4)  # æœªå‘½ä¸­GTï¼Œæ”¶æ•›ä¿å®ˆ
                logger.info(f"å¢å¼ºç‰ˆanswer_relevancy: {score:.4f}")
                return {
                    'score': score,
                    'explanation': explanation,
                    'llm_parsing_success': True,
                    'fallback_method': None
                }
            except:
                # ç®€å•çš„ç›¸å…³æ€§æ£€æŸ¥
                question_words = set(sample.user_input.lower().split())
                answer_words = set(sample.response.lower().split())
                
                # è®¡ç®—è¯æ±‡é‡å åº¦
                overlap = len(question_words & answer_words)
                total_question_words = len(question_words)
                
                if total_question_words > 0:
                    score = min(overlap / total_question_words, 1.0)
                else:
                    score = 0.0
                # ç»“åˆæ ‡å‡†ç­”æ¡ˆè¿›è¡Œå†æ ¡å‡†
                if sample.reference and isinstance(sample.reference, str):
                    ref = sample.reference.strip()
                    if ref:
                        if ref in (sample.response or ""):
                            score = max(score, 0.8)
                        else:
                            score = min(score, 0.4)
                
                logger.info(f"å¢å¼ºç‰ˆanswer_relevancy (å¯å‘å¼): {score:.4f}")
                return {
                    'score': score,
                    'explanation': f'å¯å‘å¼è¯„ä¼°: è¯æ±‡é‡å åº¦ {overlap}/{total_question_words}',
                    'llm_parsing_success': False,
                    'fallback_method': 'heuristic'
                }
                
        except Exception as e:
            logger.error(f"å¢å¼ºç‰ˆanswer_relevancyè¯„ä¼°å¤±è´¥: {e}")
            return {
                'score': 0.0,
                'explanation': f"è¯„ä¼°å¤±è´¥: {str(e)}",
                'llm_parsing_success': False,
                'fallback_method': 'error_fallback'
            }

    async def evaluate_sample(self, sample: SingleTurnSample) -> Dict[str, float]:
        """è¯„æµ‹å•ä¸ªæ ·æœ¬"""
        try:
            logger.info(f"è¯„ä¼°æ ·æœ¬: {sample.user_input[:50]}...")
            
            results = {}
            
            # ä½¿ç”¨å¢å¼ºç‰ˆè¯„ä¼°æ–¹æ³•
            if self.evaluation_config.enable_faithfulness:
                faithfulness_result = await self._evaluate_faithfulness_enhanced(sample)
                results['faithfulness'] = faithfulness_result['score']
            
            if self.evaluation_config.enable_context_precision:
                precision_result = await self._evaluate_context_precision_enhanced(sample)
                results['context_precision'] = precision_result['score']
            
            if self.evaluation_config.enable_context_recall:
                recall_result = await self._evaluate_context_recall_enhanced(sample)
                results['context_recall'] = recall_result['score']
            
            if self.evaluation_config.enable_answer_relevancy:
                relevancy_result = await self._evaluate_answer_relevancy_enhanced(sample)
                results['answer_relevancy'] = relevancy_result['score']
            
            return results
            
        except Exception as e:
            logger.error(f"æ ·æœ¬è¯„ä¼°å¤±è´¥: {e}")
            return {
                'faithfulness': 0.0,
                'answer_relevancy': 0.0,
                'context_precision': 0.0,
                'context_recall': 0.0
            }

    async def evaluate_batch(self, samples: List[SingleTurnSample]) -> List[Dict[str, float]]:
        """æ‰¹é‡è¯„æµ‹æ ·æœ¬"""
        if not samples:
            return []
        
        results = []
        for i, sample in enumerate(samples):
            logger.info(f"è¯„ä¼°æ ·æœ¬ {i+1}/{len(samples)}")
            result = await self.evaluate_sample(sample)
            results.append(result)
        
        return results

    async def evaluate_with_detailed_results(self, data: Dict[str, Any]) -> EvaluationResult:
        """è¯„æµ‹æ ·æœ¬å¹¶è¿”å›è¯¦ç»†ç»“æœ"""
        try:
            sample = self.create_sample(data)
            sample_id = data.get('id', f"sample_{hash(str(data))}")
            
            # æ‰§è¡Œè¯„æµ‹
            metrics = await self.evaluate_sample(sample)
            
            # æ”¶é›†è¯¦ç»†çš„è¿‡ç¨‹æ•°æ®
            process_data = {
                'question': sample.user_input,
                'answer': sample.response,
                'contexts': sample.retrieved_contexts,
                'ground_truth': sample.reference,
                'evaluation_config': {
                    'enable_faithfulness': self.evaluation_config.enable_faithfulness,
                    'enable_context_precision': self.evaluation_config.enable_context_precision,
                    'enable_context_recall': self.evaluation_config.enable_context_recall,
                    'enable_answer_relevancy': self.evaluation_config.enable_answer_relevancy,
                    'use_enhanced_methods': self.evaluation_config.use_enhanced_methods,
                    'chinese_optimization': self.evaluation_config.chinese_optimization,
                    'medical_domain': self.evaluation_config.medical_domain
                }
            }
            
            # åŒ»å­¦æœ¯è¯­åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
            medical_term_analysis = {
                'medical_keywords_found': len([word for word in ['æ‚£è€…', 'æ²»ç–—', 'è¯Šæ–­', 'ç—‡çŠ¶', 'è¯ç‰©'] 
                                             if word in sample.user_input or word in sample.response]),
                'chinese_medical_terms': [word for word in sample.user_input.split() 
                                        if len(word) > 2 and any(char in word for char in 'ç—…ç—›ç—‡æ²»è¯')]
            }
            
            # ä¸­æ–‡å¤„ç†ä¿¡æ¯
            chinese_processing_info = {
                'is_chinese_content': any('\u4e00' <= char <= '\u9fff' for char in sample.user_input),
                'chinese_character_count': sum(1 for char in sample.user_input if '\u4e00' <= char <= '\u9fff'),
                'processing_method': 'enhanced_chinese_processing' if self.evaluation_config.chinese_optimization else 'standard'
            }
            
            return EvaluationResult(
                sample_id=sample_id,
                metrics=metrics,
                enhanced_metrics=metrics,  # åœ¨å¢å¼ºç‰ˆä¸­ï¼Œmetrics å’Œ enhanced_metrics ç›¸åŒ
                llm_parsing_success=True,  # ç®€åŒ–å¤„ç†
                fallback_method_used=None,
                medical_term_analysis=medical_term_analysis,
                chinese_processing_info=chinese_processing_info,
                process_data=process_data,
                status=EvaluationStatus.COMPLETED,
                error_message=None,
                created_at=datetime.now().isoformat()
            )
            
        except Exception as e:
            logger.error(f"è¯¦ç»†è¯„æµ‹å¤±è´¥: {e}")
            return EvaluationResult(
                sample_id=data.get('id', 'unknown'),
                metrics={},
                enhanced_metrics={},
                llm_parsing_success=False,
                fallback_method_used='error_fallback',
                medical_term_analysis={},
                chinese_processing_info={},
                process_data={},
                status=EvaluationStatus.FAILED,
                error_message=str(e),
                created_at=datetime.now().isoformat()
            )


# å…¼å®¹æ€§å‡½æ•°ï¼Œä¿æŒä¸åŸæœ‰ä»£ç çš„å…¼å®¹æ€§
def create_enhanced_evaluator() -> EnhancedRAGASEvaluator:
    """åˆ›å»ºå¢å¼ºç‰ˆè¯„ä¼°å™¨å®ä¾‹"""
    return EnhancedRAGASEvaluator()


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    # æµ‹è¯•å¢å¼ºç‰ˆè¯„ä¼°å™¨
    print("ğŸš€ æµ‹è¯•å¢å¼ºç‰ˆRAGASè¯„ä¼°å™¨")
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
    
    try:
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = create_enhanced_evaluator()
        
        # æµ‹è¯•æ•°æ®
        test_data = {
            "id": "test_001",
            "question": "ç³–å°¿ç—…æ‚£è€…çš„é¥®é£Ÿç®¡ç†å»ºè®®ï¼Ÿ",
            "answer": "ç³–å°¿ç—…æ‚£è€…é¥®é£Ÿç®¡ç†ï¼š1. æ§åˆ¶æ€»çƒ­é‡ 2. åˆç†åˆ†é…ä¸‰å¤§è¥å…»ç´  3. å®šæ—¶å®šé‡è¿›é¤",
            "contexts": [
                "ç³–å°¿ç—…éœ€è¦ä¸¥æ ¼çš„é¥®é£Ÿæ§åˆ¶",
                "è¥å…»å‡è¡¡å¯¹è¡€ç³–æ§åˆ¶å¾ˆé‡è¦"
            ],
            "ground_truth": "ç³–å°¿ç—…æ‚£è€…åº”è¯¥æ§åˆ¶é¥®é£Ÿ"
        }
        
        print(f"\nğŸ“Š æµ‹è¯•æ•°æ®:")
        print(f"   é—®é¢˜: {test_data['question']}")
        print(f"   ç­”æ¡ˆ: {test_data['answer']}")
        print(f"   ä¸Šä¸‹æ–‡æ•°é‡: {len(test_data['contexts'])}")
        
        # è¿è¡Œè¯„ä¼°
        result = await evaluator.evaluate_with_detailed_results(test_data)
        
        print(f"\nâœ… è¯„ä¼°å®Œæˆï¼")
        print(f"è¯„æµ‹ç»“æœ:")
        print(f"  çŠ¶æ€: {result.status.value}")
        print(f"  æŒ‡æ ‡åˆ†æ•°:")
        for metric, score in result.metrics.items():
            status = "âœ…" if score > 0 else "âš ï¸"
            print(f"    {status} {metric}: {score:.4f}")
        
        print(f"  åŒ»å­¦æœ¯è¯­åˆ†æ: {result.medical_term_analysis}")
        print(f"  ä¸­æ–‡å¤„ç†ä¿¡æ¯: {result.chinese_processing_info}")
        
        if result.status == EvaluationStatus.COMPLETED:
            print(f"\nğŸ‰ æµ‹è¯•æˆåŠŸï¼å¢å¼ºç‰ˆè¯„ä¼°å™¨å·¥ä½œæ­£å¸¸")
        else:
            print(f"\nâŒ æµ‹è¯•å¤±è´¥: {result.error_message}")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        logger.error(f"è¯¦ç»†é”™è¯¯: {e}", exc_info=True)

if __name__ == "__main__":
    asyncio.run(main())
