#!/usr/bin/env python3
"""
æ¨ç†æ•°æ® RAGAS è¯„ä¼°å™¨
ä¸“é—¨è¯„æµ‹æ£€æŸ¥é¡¹ç›®æ¨èæ¨ç†æ•°æ®ï¼Œä½¿ç”¨çœŸå®çš„ RAGAS è¯„æµ‹æ–¹æ¡ˆ
"""
import os
import json
import logging
import asyncio
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass
from enum import Enum
from datetime import datetime

try:
    from ragas.dataset_schema import SingleTurnSample
    from ragas.metrics import Faithfulness, ContextPrecision, ContextRecall, AnswerRelevancy
    from langchain_openai import ChatOpenAI, OpenAIEmbeddings
    RAGAS_AVAILABLE = True
except ImportError as e:
    logging.warning(f"RAGASç›¸å…³ä¾èµ–æœªå®‰è£…: {e}")
    RAGAS_AVAILABLE = False

logger = logging.getLogger(__name__)


class EvaluationStatus(Enum):
    """è¯„æµ‹çŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"


@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    llm_model: str = "Qwen/Qwen2.5-32B-Instruct"
    embedding_model: str = "BAAI/bge-m3"
    api_key: str = ""
    base_url: str = "https://api.siliconflow.cn/v1"
    temperature: float = 0.1
    timeout: int = 60
    max_retries: int = 2


@dataclass
class EvaluationResult:
    """è¯„æµ‹ç»“æœ"""
    sample_id: str
    metrics: Dict[str, float]
    status: EvaluationStatus
    error_message: Optional[str] = None
    processing_info: Dict[str, Any] = None
    created_at: str = ""


class InferenceRAGASEvaluator:
    """æ¨ç†æ•°æ® RAGAS è¯„ä¼°å™¨"""
    
    def __init__(self, model_config: Optional[ModelConfig] = None):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        if not RAGAS_AVAILABLE:
            raise ImportError("RAGASç›¸å…³ä¾èµ–æœªå®‰è£…")
        
        # è®¾ç½®äº‹ä»¶å¾ªç¯å…¼å®¹æ€§
        self._setup_event_loop()
        
        # åŠ è½½é…ç½®
        self.model_config = model_config or self._load_default_config()
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._init_models()
        
        # åˆå§‹åŒ–è¯„æµ‹æŒ‡æ ‡
        self._init_metrics()
        
        logger.info(f"æ¨ç†æ•°æ® RAGAS è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ - LLM: {self.model_config.llm_model}")
    
    def _setup_event_loop(self):
        """è®¾ç½®äº‹ä»¶å¾ªç¯å…¼å®¹æ€§"""
        try:
            asyncio.set_event_loop_policy(asyncio.DefaultEventLoopPolicy())
        except Exception as e:
            logger.warning(f"äº‹ä»¶å¾ªç¯è®¾ç½®å¤±è´¥: {e}")
    
    def _load_default_config(self) -> ModelConfig:
        """åŠ è½½é»˜è®¤é…ç½®"""
        api_key = os.getenv("SILICONFLOW_API_KEY") or os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("æœªæ‰¾åˆ°APIå¯†é’¥ï¼Œè¯·è®¾ç½® SILICONFLOW_API_KEY")
        
        return ModelConfig(
            api_key=api_key,
            base_url=os.getenv("SILICONFLOW_BASE_URL", "https://api.siliconflow.cn/v1"),
            llm_model=os.getenv("SILICONFLOW_LLM_MODEL", "Qwen/Qwen2.5-32B-Instruct"),
            embedding_model=os.getenv("SILICONFLOW_EMBEDDING_MODEL", "BAAI/bge-m3")
        )
    
    def _init_models(self):
        """åˆå§‹åŒ–LLMå’ŒåµŒå…¥æ¨¡å‹"""
        self.llm = ChatOpenAI(
            model=self.model_config.llm_model,
            api_key=self.model_config.api_key,
            base_url=self.model_config.base_url,
            temperature=self.model_config.temperature,
            timeout=self.model_config.timeout,
            max_retries=self.model_config.max_retries
        )
        
        self.embeddings = OpenAIEmbeddings(
            model=self.model_config.embedding_model,
            api_key=self.model_config.api_key,
            base_url=self.model_config.base_url,
            timeout=self.model_config.timeout,
            max_retries=self.model_config.max_retries
        )
    
    def _init_metrics(self):
        """åˆå§‹åŒ–è¯„æµ‹æŒ‡æ ‡"""
        # ä½¿ç”¨é»˜è®¤çš„ LLM å’Œ embeddings
        self.metrics = {
            'faithfulness': Faithfulness(),
            'context_precision': ContextPrecision(),
            'context_recall': ContextRecall(),
            'answer_relevancy': AnswerRelevancy()
        }
        
        logger.info(f"åˆå§‹åŒ–è¯„æµ‹æŒ‡æ ‡: {list(self.metrics.keys())}")
    
    def create_sample(self, data: Dict[str, Any]) -> SingleTurnSample:
        """åˆ›å»ºRAGASæ ·æœ¬å¯¹è±¡"""
        question = str(data.get('question', ''))
        answer = str(data.get('answer', ''))
        contexts = data.get('contexts', [])
        ground_truth = str(data.get('ground_truth', ''))
        
        # ç¡®ä¿contextsæ˜¯å­—ç¬¦ä¸²åˆ—è¡¨
        if isinstance(contexts, str):
            contexts = [contexts]
        elif not isinstance(contexts, list):
            contexts = [str(contexts)]
        
        # è¿‡æ»¤ç©ºçš„ä¸Šä¸‹æ–‡
        contexts = [ctx for ctx in contexts if ctx and str(ctx).strip()]
        if not contexts:
            contexts = ["ç›¸å…³æ£€æŸ¥é¡¹ç›®çŸ¥è¯†"]  # é»˜è®¤ä¸Šä¸‹æ–‡
        
        return SingleTurnSample(
            user_input=question,
            response=answer,
            retrieved_contexts=contexts,
            reference=ground_truth
        )
    
    async def evaluate_sample(self, data: Dict[str, Any]) -> EvaluationResult:
        """è¯„æµ‹å•ä¸ªæ ·æœ¬"""
        try:
            sample = self.create_sample(data)
            sample_id = data.get('id', f"sample_{hash(str(data))}")
            
            logger.info(f"å¼€å§‹è¯„æµ‹æ ·æœ¬: {sample_id}")
            
            # ä½¿ç”¨çœŸå®çš„ RAGAS è¯„æµ‹æ–¹æ³•
            results = {}
            
            # è¯„æµ‹å¿ å®åº¦
            try:
                # ä½¿ç”¨ RAGAS çš„ evaluate æ–¹æ³•
                from ragas import evaluate
                faithfulness_result = await evaluate(
                    [sample],
                    metrics=[self.metrics['faithfulness']]
                )
                logger.info(f"å¿ å®åº¦è¯„æµ‹åŸå§‹ç»“æœ: {type(faithfulness_result)}, {faithfulness_result}")
                # ä»ç»“æœä¸­æå–åˆ†æ•°
                if 'faithfulness' in faithfulness_result:
                    results['faithfulness'] = float(faithfulness_result['faithfulness'][0])
                else:
                    results['faithfulness'] = 0.0
                logger.info(f"å¿ å®åº¦è¯„æµ‹å®Œæˆ: {results['faithfulness']:.4f}")
            except Exception as e:
                logger.error(f"å¿ å®åº¦è¯„æµ‹å¤±è´¥: {e}")
                results['faithfulness'] = 0.0
            
            # è¯„æµ‹ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦
            try:
                context_precision_result = await evaluate(
                    [sample],
                    metrics=[self.metrics['context_precision']]
                )
                if 'context_precision' in context_precision_result:
                    results['context_precision'] = float(context_precision_result['context_precision'][0])
                else:
                    results['context_precision'] = 0.0
                logger.info(f"ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦è¯„æµ‹å®Œæˆ: {results['context_precision']:.4f}")
            except Exception as e:
                logger.error(f"ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦è¯„æµ‹å¤±è´¥: {e}")
                results['context_precision'] = 0.0
            
            # è¯„æµ‹ä¸Šä¸‹æ–‡å¬å›ç‡
            try:
                context_recall_result = await evaluate(
                    [sample],
                    metrics=[self.metrics['context_recall']]
                )
                if 'context_recall' in context_recall_result:
                    results['context_recall'] = float(context_recall_result['context_recall'][0])
                else:
                    results['context_recall'] = 0.0
                logger.info(f"ä¸Šä¸‹æ–‡å¬å›ç‡è¯„æµ‹å®Œæˆ: {results['context_recall']:.4f}")
            except Exception as e:
                logger.error(f"ä¸Šä¸‹æ–‡å¬å›ç‡è¯„æµ‹å¤±è´¥: {e}")
                results['context_recall'] = 0.0
            
            # è¯„æµ‹ç­”æ¡ˆç›¸å…³æ€§
            try:
                answer_relevancy_result = await evaluate(
                    [sample],
                    metrics=[self.metrics['answer_relevancy']]
                )
                if 'answer_relevancy' in answer_relevancy_result:
                    results['answer_relevancy'] = float(answer_relevancy_result['answer_relevancy'][0])
                else:
                    results['answer_relevancy'] = 0.0
                logger.info(f"ç­”æ¡ˆç›¸å…³æ€§è¯„æµ‹å®Œæˆ: {results['answer_relevancy']:.4f}")
            except Exception as e:
                logger.error(f"ç­”æ¡ˆç›¸å…³æ€§è¯„æµ‹å¤±è´¥: {e}")
                results['answer_relevancy'] = 0.0
            
            # æ”¶é›†å¤„ç†ä¿¡æ¯
            processing_info = {
                'sample_data': data,
                'sample_created': True,
                'metrics_evaluated': list(results.keys()),
                'evaluation_timestamp': datetime.now().isoformat()
            }
            
            return EvaluationResult(
                sample_id=sample_id,
                metrics=results,
                status=EvaluationStatus.COMPLETED,
                processing_info=processing_info,
                created_at=datetime.now().isoformat()
            )
            
        except Exception as e:
            logger.error(f"æ ·æœ¬è¯„æµ‹å¤±è´¥: {e}")
            return EvaluationResult(
                sample_id=data.get('id', 'unknown'),
                metrics={},
                status=EvaluationStatus.FAILED,
                error_message=str(e),
                processing_info={'error': str(e)},
                created_at=datetime.now().isoformat()
            )
    
    async def evaluate_batch(self, data_list: List[Dict[str, Any]]) -> List[EvaluationResult]:
        """æ‰¹é‡è¯„æµ‹"""
        if not data_list:
            return []
        
        results = []
        for i, data in enumerate(data_list):
            logger.info(f"è¯„æµ‹æ ·æœ¬ {i+1}/{len(data_list)}")
            result = await self.evaluate_sample(data)
            results.append(result)
        
        return results
    
    async def get_evaluation_statistics(self, results: List[EvaluationResult]) -> Dict[str, Any]:
        """è·å–è¯„æµ‹ç»Ÿè®¡ä¿¡æ¯"""
        if not results:
            return {}
        
        # è¿‡æ»¤æˆåŠŸçš„è¯„æµ‹ç»“æœ
        successful_results = [r for r in results if r.status == EvaluationStatus.COMPLETED]
        
        if not successful_results:
            return {
                'total_samples': len(results),
                'successful_evaluations': 0,
                'failed_evaluations': len(results),
                'success_rate': 0.0
            }
        
        # è®¡ç®—å„æŒ‡æ ‡çš„å¹³å‡åˆ†
        metrics_stats = {}
        for metric in ['faithfulness', 'context_precision', 'context_recall', 'answer_relevancy']:
            scores = [r.metrics.get(metric, 0.0) for r in successful_results if metric in r.metrics]
            if scores:
                metrics_stats[metric] = {
                    'mean': sum(scores) / len(scores),
                    'min': min(scores),
                    'max': max(scores),
                    'count': len(scores)
                }
        
        return {
            'total_samples': len(results),
            'successful_evaluations': len(successful_results),
            'failed_evaluations': len(results) - len(successful_results),
            'success_rate': len(successful_results) / len(results),
            'metrics_statistics': metrics_stats
        }


# å…¼å®¹æ€§å‡½æ•°
def create_inference_ragas_evaluator(model_config: Optional[ModelConfig] = None) -> InferenceRAGASEvaluator:
    """åˆ›å»ºæ¨ç†æ•°æ® RAGAS è¯„ä¼°å™¨å®ä¾‹"""
    return InferenceRAGASEvaluator(model_config)


if __name__ == "__main__":
    # æµ‹è¯•æ¨ç†æ•°æ® RAGAS è¯„ä¼°å™¨
    print("ğŸš€ æµ‹è¯•æ¨ç†æ•°æ® RAGAS è¯„ä¼°å™¨")
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
    
    async def test_evaluator():
        try:
            # åˆ›å»ºè¯„ä¼°å™¨
            evaluator = create_inference_ragas_evaluator()
            print("âœ… æ¨ç†æ•°æ® RAGAS è¯„ä¼°å™¨åˆ›å»ºæˆåŠŸ")
            
            # æµ‹è¯•æ•°æ® - æ£€æŸ¥é¡¹ç›®æ¨èæ¨ç†æ•°æ®
            test_data = {
                "id": "inference_001",
                "question": "æ‚£è€…å‡ºç°èƒ¸ç—›ç—‡çŠ¶ï¼Œéœ€è¦æ¨èå“ªäº›æ£€æŸ¥é¡¹ç›®ï¼Ÿ",
                "answer": "å»ºè®®è¿›è¡Œä»¥ä¸‹æ£€æŸ¥ï¼š1. å¿ƒç”µå›¾ 2. èƒ¸éƒ¨Xå…‰ 3. å¿ƒè‚Œé…¶è°± 4. è¡€å¸¸è§„",
                "contexts": [
                    "èƒ¸ç—›æ˜¯å¸¸è§ç—‡çŠ¶ï¼Œéœ€è¦æ’é™¤å¿ƒè„ç–¾ç—…",
                    "å¿ƒç”µå›¾å¯ä»¥æ£€æµ‹å¿ƒå¾‹å¼‚å¸¸",
                    "èƒ¸éƒ¨Xå…‰å¯ä»¥è§‚å¯Ÿè‚ºéƒ¨æƒ…å†µ"
                ],
                "ground_truth": "èƒ¸ç—›æ‚£è€…åº”è¿›è¡Œå¿ƒç”µå›¾ã€èƒ¸éƒ¨Xå…‰ã€å¿ƒè‚Œé…¶è°±æ£€æŸ¥"
            }
            
            print(f"\nğŸ“Š æµ‹è¯•æ•°æ®:")
            print(f"   é—®é¢˜: {test_data['question']}")
            print(f"   ç­”æ¡ˆ: {test_data['answer']}")
            print(f"   ä¸Šä¸‹æ–‡æ•°é‡: {len(test_data['contexts'])}")
            
            # è¿è¡Œè¯„æµ‹
            result = await evaluator.evaluate_sample(test_data)
            
            print(f"\nâœ… è¯„æµ‹å®Œæˆï¼")
            print(f"è¯„æµ‹ç»“æœ:")
            print(f"  çŠ¶æ€: {result.status.value}")
            print(f"  æŒ‡æ ‡åˆ†æ•°:")
            
            total_score = 0
            valid_metrics = 0
            for metric, score in result.metrics.items():
                status = "âœ…" if score > 0 else "âš ï¸"
                print(f"    {status} {metric}: {score:.4f}")
                if score > 0:
                    total_score += score
                    valid_metrics += 1
            
            if valid_metrics > 0:
                avg_score = total_score / valid_metrics
                print(f"\nğŸ“Š å¹³å‡åˆ†: {avg_score:.4f} (æœ‰æ•ˆæŒ‡æ ‡: {valid_metrics}/4)")
                
                if valid_metrics >= 3:
                    print(f"\nğŸ‰ æµ‹è¯•æˆåŠŸï¼æ¨ç†æ•°æ® RAGAS è¯„ä¼°å™¨å·¥ä½œæ­£å¸¸")
                else:
                    print(f"\nâš ï¸  éƒ¨åˆ†æŒ‡æ ‡ä»éœ€ä¼˜åŒ–")
            else:
                print(f"\nâŒ æ‰€æœ‰æŒ‡æ ‡éƒ½ä¸º0ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            logger.error(f"è¯¦ç»†é”™è¯¯: {e}", exc_info=True)
    
    asyncio.run(test_evaluator())
