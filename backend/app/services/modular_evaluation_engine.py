#!/usr/bin/env python3
"""
æ¨¡å—åŒ–è¯„æµ‹å¼•æ“æ¥å£
æä¾›å¯æ‰©å±•çš„è¯„æµ‹å¼•æ“æ¶æ„ï¼Œæ”¯æŒæ’ä»¶åŒ–çš„è¯„æµ‹æŒ‡æ ‡ç®¡ç†
"""
import os
import json
import logging
import asyncio
from typing import Dict, List, Any, Optional, Union, Protocol, Type
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime
import numpy as np

try:
    from ragas.dataset_schema import SingleTurnSample
    from ragas.metrics import Faithfulness, ContextPrecision, ContextRecall
    from langchain_openai import ChatOpenAI, OpenAIEmbeddings
    RAGAS_AVAILABLE = True
except ImportError as e:
    logging.warning(f"RAGASç›¸å…³ä¾èµ–æœªå®‰è£…: {e}")
    RAGAS_AVAILABLE = False

logger = logging.getLogger(__name__)


class EvaluationStatus(Enum):
    """è¯„æµ‹çŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"


@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    llm_model: str = "Qwen/Qwen2.5-32B-Instruct"
    embedding_model: str = "BAAI/bge-m3"
    rerank_model: Optional[str] = None
    api_key: str = ""
    base_url: str = "https://api.siliconflow.cn/v1"
    temperature: float = 0.1
    timeout: int = 60
    max_retries: int = 2


@dataclass
class EvaluationConfig:
    """è¯„æµ‹é…ç½®"""
    enable_faithfulness: bool = True
    enable_context_precision: bool = True
    enable_context_recall: bool = True
    enable_answer_relevancy: bool = True
    use_enhanced_methods: bool = True
    chinese_optimization: bool = True
    medical_domain: bool = True
    custom_metrics: List[str] = field(default_factory=list)


@dataclass
class MetricInfo:
    """è¯„æµ‹æŒ‡æ ‡ä¿¡æ¯"""
    name: str
    description: str
    enabled: bool = True
    weight: float = 1.0
    category: str = "default"
    dependencies: List[str] = field(default_factory=list)


@dataclass
class EvaluationResult:
    """è¯„æµ‹ç»“æœ"""
    sample_id: str
    metrics: Dict[str, float]
    enhanced_metrics: Dict[str, float]
    llm_parsing_success: bool
    fallback_method_used: Optional[str]
    medical_term_analysis: Dict[str, Any]
    chinese_processing_info: Dict[str, Any]
    process_data: Dict[str, Any]
    status: EvaluationStatus
    error_message: Optional[str]
    created_at: str


class MetricEvaluator(Protocol):
    """è¯„æµ‹æŒ‡æ ‡æ¥å£åè®®"""
    
    async def evaluate(self, sample: SingleTurnSample) -> Dict[str, Any]:
        """è¯„æµ‹å•ä¸ªæ ·æœ¬"""
        ...
    
    def get_metric_info(self) -> MetricInfo:
        """è·å–æŒ‡æ ‡ä¿¡æ¯"""
        ...


class BaseEvaluationEngine(ABC):
    """è¯„æµ‹å¼•æ“åŸºç±»"""
    
    def __init__(self, model_config: ModelConfig, evaluation_config: EvaluationConfig):
        self.model_config = model_config
        self.evaluation_config = evaluation_config
        self.metrics: Dict[str, MetricEvaluator] = {}
        self._initialized = False
    
    @abstractmethod
    async def initialize(self) -> None:
        """åˆå§‹åŒ–è¯„æµ‹å¼•æ“"""
        pass
    
    @abstractmethod
    async def evaluate_sample(self, sample: SingleTurnSample) -> Dict[str, float]:
        """è¯„æµ‹å•ä¸ªæ ·æœ¬"""
        pass
    
    @abstractmethod
    async def evaluate_batch(self, samples: List[SingleTurnSample]) -> List[Dict[str, float]]:
        """æ‰¹é‡è¯„æµ‹æ ·æœ¬"""
        pass
    
    @abstractmethod
    async def get_available_metrics(self) -> List[MetricInfo]:
        """è·å–å¯ç”¨çš„è¯„æµ‹æŒ‡æ ‡"""
        pass
    
    @abstractmethod
    async def update_config(self, config: EvaluationConfig) -> None:
        """æ›´æ–°è¯„æµ‹é…ç½®"""
        pass


class ModularEvaluationEngine(BaseEvaluationEngine):
    """æ¨¡å—åŒ–è¯„æµ‹å¼•æ“"""
    
    def __init__(self, model_config: ModelConfig, evaluation_config: EvaluationConfig):
        super().__init__(model_config, evaluation_config)
        self.llm = None
        self.embeddings = None
        self.rerank_model = None
    
    async def initialize(self) -> None:
        """åˆå§‹åŒ–è¯„æµ‹å¼•æ“"""
        if not RAGAS_AVAILABLE:
            raise ImportError("RAGASç›¸å…³ä¾èµ–æœªå®‰è£…")
        
        # åˆå§‹åŒ–æ¨¡å‹
        await self._init_models()
        
        # åˆå§‹åŒ–è¯„æµ‹æŒ‡æ ‡
        await self._init_metrics()
        
        self._initialized = True
        logger.info("æ¨¡å—åŒ–è¯„æµ‹å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    async def _init_models(self) -> None:
        """åˆå§‹åŒ–æ¨¡å‹"""
        self.llm = ChatOpenAI(
            model=self.model_config.llm_model,
            api_key=self.model_config.api_key,
            base_url=self.model_config.base_url,
            temperature=self.model_config.temperature,
            timeout=self.model_config.timeout,
            max_retries=self.model_config.max_retries
        )
        
        self.embeddings = OpenAIEmbeddings(
            model=self.model_config.embedding_model,
            api_key=self.model_config.api_key,
            base_url=self.model_config.base_url,
            timeout=self.model_config.timeout,
            max_retries=self.model_config.max_retries
        )
        
        # å¦‚æœæœ‰é‡æ’åºæ¨¡å‹ï¼Œåˆå§‹åŒ–å®ƒ
        if self.model_config.rerank_model:
            # è¿™é‡Œå¯ä»¥æ·»åŠ é‡æ’åºæ¨¡å‹çš„åˆå§‹åŒ–é€»è¾‘
            logger.info(f"é‡æ’åºæ¨¡å‹é…ç½®: {self.model_config.rerank_model}")
    
    async def _init_metrics(self) -> None:
        """åˆå§‹åŒ–è¯„æµ‹æŒ‡æ ‡"""
        self.metrics = {}
        
        if self.evaluation_config.enable_faithfulness:
            self.metrics['faithfulness'] = Faithfulness()
        
        if self.evaluation_config.enable_context_precision:
            self.metrics['context_precision'] = ContextPrecision()
        
        if self.evaluation_config.enable_context_recall:
            self.metrics['context_recall'] = ContextRecall()
        
        # æ·»åŠ è‡ªå®šä¹‰æŒ‡æ ‡
        for metric_name in self.evaluation_config.custom_metrics:
            await self._load_custom_metric(metric_name)
    
    async def _load_custom_metric(self, metric_name: str) -> None:
        """åŠ è½½è‡ªå®šä¹‰æŒ‡æ ‡"""
        # è¿™é‡Œå¯ä»¥å®ç°è‡ªå®šä¹‰æŒ‡æ ‡çš„åŠ è½½é€»è¾‘
        logger.info(f"åŠ è½½è‡ªå®šä¹‰æŒ‡æ ‡: {metric_name}")
    
    async def evaluate_sample(self, sample: SingleTurnSample) -> Dict[str, float]:
        """è¯„æµ‹å•ä¸ªæ ·æœ¬"""
        if not self._initialized:
            await self.initialize()
        
        results = {}
        
        for metric_name, metric_evaluator in self.metrics.items():
            try:
                if hasattr(metric_evaluator, 'ascore'):
                    # ä½¿ç”¨ RAGAS çš„å¼‚æ­¥è¯„åˆ†æ–¹æ³•
                    score = await metric_evaluator.ascore(sample)
                elif hasattr(metric_evaluator, 'score'):
                    # ä½¿ç”¨ RAGAS çš„åŒæ­¥è¯„åˆ†æ–¹æ³•
                    score = metric_evaluator.score(sample)
                else:
                    # è‡ªå®šä¹‰è¯„æµ‹æ–¹æ³•
                    score = await self._evaluate_custom_metric(metric_evaluator, sample)
                
                results[metric_name] = float(score) if score is not None else 0.0
                logger.debug(f"æŒ‡æ ‡ {metric_name} è¯„æµ‹å®Œæˆ: {results[metric_name]}")
                
            except Exception as e:
                logger.error(f"æŒ‡æ ‡ {metric_name} è¯„æµ‹å¤±è´¥: {e}")
                results[metric_name] = 0.0
        
        return results
    
    async def _evaluate_custom_metric(self, metric_evaluator: MetricEvaluator, sample: SingleTurnSample) -> float:
        """è¯„æµ‹è‡ªå®šä¹‰æŒ‡æ ‡"""
        try:
            result = await metric_evaluator.evaluate(sample)
            return result.get('score', 0.0)
        except Exception as e:
            logger.error(f"è‡ªå®šä¹‰æŒ‡æ ‡è¯„æµ‹å¤±è´¥: {e}")
            return 0.0
    
    async def evaluate_batch(self, samples: List[SingleTurnSample]) -> List[Dict[str, float]]:
        """æ‰¹é‡è¯„æµ‹æ ·æœ¬"""
        if not self._initialized:
            await self.initialize()
        
        results = []
        for i, sample in enumerate(samples):
            logger.info(f"è¯„æµ‹æ ·æœ¬ {i+1}/{len(samples)}")
            result = await self.evaluate_sample(sample)
            results.append(result)
        
        return results
    
    async def get_available_metrics(self) -> List[MetricInfo]:
        """è·å–å¯ç”¨çš„è¯„æµ‹æŒ‡æ ‡"""
        metrics_info = []
        
        # æ ‡å‡† RAGAS æŒ‡æ ‡
        standard_metrics = {
            'faithfulness': MetricInfo(
                name='faithfulness',
                description='å¿ å®åº¦ï¼šè¯„ä¼°ç­”æ¡ˆä¸ç»™å®šä¸Šä¸‹æ–‡çš„äº‹å®ä¸€è‡´æ€§',
                enabled=self.evaluation_config.enable_faithfulness,
                category='ragas_standard'
            ),
            'context_precision': MetricInfo(
                name='context_precision',
                description='ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦ï¼šè¯„ä¼°æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸é—®é¢˜çš„ç›¸å…³ç¨‹åº¦',
                enabled=self.evaluation_config.enable_context_precision,
                category='ragas_standard'
            ),
            'context_recall': MetricInfo(
                name='context_recall',
                description='ä¸Šä¸‹æ–‡å¬å›ç‡ï¼šè¯„ä¼°æ£€ç´¢å™¨æ£€ç´¢æ‰€æœ‰å¿…è¦ä¿¡æ¯çš„èƒ½åŠ›',
                enabled=self.evaluation_config.enable_context_recall,
                category='ragas_standard'
            ),
            'answer_relevancy': MetricInfo(
                name='answer_relevancy',
                description='ç­”æ¡ˆç›¸å…³æ€§ï¼šè¯„ä¼°ç”Ÿæˆçš„ç­”æ¡ˆä¸ç”¨æˆ·é—®é¢˜çš„ç›¸å…³ç¨‹åº¦',
                enabled=self.evaluation_config.enable_answer_relevancy,
                category='ragas_standard'
            )
        }
        
        for metric_name, metric_info in standard_metrics.items():
            if metric_name in self.metrics:
                metrics_info.append(metric_info)
        
        # è‡ªå®šä¹‰æŒ‡æ ‡
        for metric_name in self.evaluation_config.custom_metrics:
            custom_metric_info = MetricInfo(
                name=metric_name,
                description=f'è‡ªå®šä¹‰æŒ‡æ ‡: {metric_name}',
                enabled=True,
                category='custom'
            )
            metrics_info.append(custom_metric_info)
        
        return metrics_info
    
    async def update_config(self, config: EvaluationConfig) -> None:
        """æ›´æ–°è¯„æµ‹é…ç½®"""
        self.evaluation_config = config
        
        # é‡æ–°åˆå§‹åŒ–æŒ‡æ ‡
        await self._init_metrics()
        
        logger.info("è¯„æµ‹é…ç½®å·²æ›´æ–°")
    
    async def add_custom_metric(self, metric_name: str, metric_evaluator: MetricEvaluator) -> None:
        """æ·»åŠ è‡ªå®šä¹‰è¯„æµ‹æŒ‡æ ‡"""
        self.metrics[metric_name] = metric_evaluator
        if metric_name not in self.evaluation_config.custom_metrics:
            self.evaluation_config.custom_metrics.append(metric_name)
        
        logger.info(f"å·²æ·»åŠ è‡ªå®šä¹‰æŒ‡æ ‡: {metric_name}")
    
    async def remove_metric(self, metric_name: str) -> None:
        """ç§»é™¤è¯„æµ‹æŒ‡æ ‡"""
        if metric_name in self.metrics:
            del self.metrics[metric_name]
        
        if metric_name in self.evaluation_config.custom_metrics:
            self.evaluation_config.custom_metrics.remove(metric_name)
        
        logger.info(f"å·²ç§»é™¤æŒ‡æ ‡: {metric_name}")
    
    async def get_metric_statistics(self, results: List[Dict[str, float]]) -> Dict[str, Dict[str, float]]:
        """è·å–è¯„æµ‹æŒ‡æ ‡ç»Ÿè®¡ä¿¡æ¯"""
        if not results:
            return {}
        
        statistics = {}
        
        # è·å–æ‰€æœ‰æŒ‡æ ‡åç§°
        all_metrics = set()
        for result in results:
            all_metrics.update(result.keys())
        
        for metric_name in all_metrics:
            scores = [result.get(metric_name, 0.0) for result in results if metric_name in result]
            
            if scores:
                statistics[metric_name] = {
                    'mean': np.mean(scores),
                    'std': np.std(scores),
                    'min': np.min(scores),
                    'max': np.max(scores),
                    'count': len(scores)
                }
        
        return statistics


class EvaluationEngineFactory:
    """è¯„æµ‹å¼•æ“å·¥å‚ç±»"""
    
    @staticmethod
    def create_engine(engine_type: str, model_config: ModelConfig, 
                     evaluation_config: EvaluationConfig) -> BaseEvaluationEngine:
        """åˆ›å»ºè¯„æµ‹å¼•æ“å®ä¾‹"""
        if engine_type == "modular":
            return ModularEvaluationEngine(model_config, evaluation_config)
        elif engine_type == "enhanced":
            # å¯¼å…¥å¢å¼ºç‰ˆè¯„ä¼°å™¨
            from .enhanced_ragas_evaluator import EnhancedRAGASEvaluator
            return EnhancedRAGASEvaluator(model_config, evaluation_config)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¯„æµ‹å¼•æ“ç±»å‹: {engine_type}")
    
    @staticmethod
    def get_available_engines() -> List[str]:
        """è·å–å¯ç”¨çš„è¯„æµ‹å¼•æ“ç±»å‹"""
        return ["modular", "enhanced"]


# å…¼å®¹æ€§å‡½æ•°
def create_modular_evaluation_engine(model_config: Optional[ModelConfig] = None,
                                   evaluation_config: Optional[EvaluationConfig] = None) -> ModularEvaluationEngine:
    """åˆ›å»ºæ¨¡å—åŒ–è¯„æµ‹å¼•æ“å®ä¾‹"""
    if model_config is None:
        # ä»ç¯å¢ƒå˜é‡åŠ è½½é»˜è®¤é…ç½®
        api_key = os.getenv("SILICONFLOW_API_KEY") or os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("æœªæ‰¾åˆ°APIå¯†é’¥ï¼Œè¯·è®¾ç½® SILICONFLOW_API_KEY")
        
        model_config = ModelConfig(
            api_key=api_key,
            base_url=os.getenv("SILICONFLOW_BASE_URL", "https://api.siliconflow.cn/v1"),
            llm_model=os.getenv("SILICONFLOW_LLM_MODEL", "Qwen/Qwen2.5-32B-Instruct"),
            embedding_model=os.getenv("SILICONFLOW_EMBEDDING_MODEL", "BAAI/bge-m3")
        )
    
    if evaluation_config is None:
        evaluation_config = EvaluationConfig()
    
    return ModularEvaluationEngine(model_config, evaluation_config)


if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å—åŒ–è¯„æµ‹å¼•æ“
    print("ğŸš€ æµ‹è¯•æ¨¡å—åŒ–è¯„æµ‹å¼•æ“")
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
    
    async def test_engine():
        try:
            # åˆ›å»ºè¯„æµ‹å¼•æ“
            engine = create_modular_evaluation_engine()
            await engine.initialize()
            print("âœ… æ¨¡å—åŒ–è¯„æµ‹å¼•æ“åˆ›å»ºæˆåŠŸ")
            
            # è·å–å¯ç”¨æŒ‡æ ‡
            metrics = await engine.get_available_metrics()
            print(f"ğŸ“Š å¯ç”¨æŒ‡æ ‡: {[m.name for m in metrics]}")
            
            # åˆ›å»ºæµ‹è¯•æ ·æœ¬
            sample = SingleTurnSample(
                user_input="ç³–å°¿ç—…æ‚£è€…çš„é¥®é£Ÿç®¡ç†å»ºè®®ï¼Ÿ",
                response="ç³–å°¿ç—…æ‚£è€…é¥®é£Ÿç®¡ç†ï¼š1. æ§åˆ¶æ€»çƒ­é‡ 2. åˆç†åˆ†é…ä¸‰å¤§è¥å…»ç´  3. å®šæ—¶å®šé‡è¿›é¤",
                retrieved_contexts=[
                    "ç³–å°¿ç—…éœ€è¦ä¸¥æ ¼çš„é¥®é£Ÿæ§åˆ¶",
                    "è¥å…»å‡è¡¡å¯¹è¡€ç³–æ§åˆ¶å¾ˆé‡è¦"
                ],
                reference="ç³–å°¿ç—…æ‚£è€…åº”è¯¥æ§åˆ¶é¥®é£Ÿ"
            )
            
            # è¯„æµ‹æ ·æœ¬
            result = await engine.evaluate_sample(sample)
            print(f"âœ… è¯„æµ‹å®Œæˆ: {result}")
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = await engine.get_metric_statistics([result])
            print(f"ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯: {stats}")
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    asyncio.run(test_engine())






















