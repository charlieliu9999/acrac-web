#!/usr/bin/env python3
"""
æµ‹è¯•RAGAS V2è¯„ä¼°å™¨ - éªŒè¯0.3.xç‰ˆæœ¬å…¼å®¹æ€§
"""
import os
import sys
import logging
import asyncio
from pathlib import Path

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['NEST_ASYNCIO_DISABLE'] = '1'
os.environ['UVLOOP_DISABLE'] = '1'

# åŠ è½½.envæ–‡ä»¶
env_file = Path(__file__).parent / '.env'
if env_file.exists():
    with open(env_file, 'r') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#') and '=' in line:
                key, value = line.split('=', 1)
                os.environ[key] = value

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

async def test_single_metric():
    """æµ‹è¯•å•ä¸ªæŒ‡æ ‡è¯„ä¼°"""
    print("=== æµ‹è¯•å•ä¸ªæŒ‡æ ‡è¯„ä¼° ===")
    
    try:
        from app.services.ragas_evaluator_v2 import RAGASEvaluatorV2
        
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = RAGASEvaluatorV2()
        print(f"âœ… è¯„ä¼°å™¨åˆ›å»ºæˆåŠŸ - LLM: {evaluator.llm_model_name}")
        
        # æµ‹è¯•æ•°æ®
        sample_data = {
            'question': "æ‚£è€…å‡ºç°èƒ¸ç—›ç—‡çŠ¶ï¼Œéœ€è¦è¿›è¡Œä»€ä¹ˆå½±åƒå­¦æ£€æŸ¥ï¼Ÿ",
            'answer': "å»ºè®®è¿›è¡Œèƒ¸éƒ¨CTæ£€æŸ¥ä»¥æ’é™¤è‚ºæ “å¡ç­‰ç–¾ç—…ã€‚",
            'contexts': ["èƒ¸ç—›æ˜¯å¸¸è§çš„ä¸´åºŠç—‡çŠ¶", "CTæ£€æŸ¥å¯ä»¥æœ‰æ•ˆè¯Šæ–­èƒ¸éƒ¨ç–¾ç—…"],
            'ground_truth': "èƒ¸éƒ¨CT"
        }
        
        # åˆ›å»ºSingleTurnSample
        sample = evaluator.create_single_turn_sample(sample_data)
        print(f"âœ… SingleTurnSampleåˆ›å»ºæˆåŠŸ")
        print(f"   ç”¨æˆ·è¾“å…¥: {sample.user_input}")
        print(f"   å“åº”: {sample.response}")
        print(f"   ä¸Šä¸‹æ–‡æ•°é‡: {len(sample.retrieved_contexts)}")
        
        # æµ‹è¯•å•ä¸ªæŒ‡æ ‡
        print("\n--- æµ‹è¯•Answer Relevancy ---")
        relevancy_score = await evaluator.answer_relevancy_metric.single_turn_ascore(sample)
        print(f"Answer Relevancy: {relevancy_score:.4f}")
        
        print("\n--- æµ‹è¯•Faithfulness ---")
        try:
            faithfulness_score = await evaluator.faithfulness_metric.single_turn_ascore(sample)
            print(f"Faithfulness: {faithfulness_score:.4f}")
        except Exception as e:
            print(f"âŒ Faithfulnessè¯„ä¼°å¤±è´¥: {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å•æŒ‡æ ‡æµ‹è¯•å¤±è´¥: {e}")
        logger.error(f"å•æŒ‡æ ‡æµ‹è¯•è¯¦ç»†é”™è¯¯: {e}", exc_info=True)
        return False

async def test_full_evaluation():
    """æµ‹è¯•å®Œæ•´è¯„ä¼°æµç¨‹"""
    print("\n=== æµ‹è¯•å®Œæ•´è¯„ä¼°æµç¨‹ ===")
    
    try:
        from app.services.ragas_evaluator_v2 import RAGASEvaluatorV2
        
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = RAGASEvaluatorV2()
        
        # æµ‹è¯•æ•°æ®
        test_samples = [
            {
                'question': "æ‚£è€…å‡ºç°èƒ¸ç—›ç—‡çŠ¶ï¼Œéœ€è¦è¿›è¡Œä»€ä¹ˆå½±åƒå­¦æ£€æŸ¥ï¼Ÿ",
                'answer': "å»ºè®®è¿›è¡Œèƒ¸éƒ¨CTæ£€æŸ¥ä»¥æ’é™¤è‚ºæ “å¡ç­‰ç–¾ç—…ã€‚",
                'contexts': ["èƒ¸ç—›æ˜¯å¸¸è§çš„ä¸´åºŠç—‡çŠ¶", "CTæ£€æŸ¥å¯ä»¥æœ‰æ•ˆè¯Šæ–­èƒ¸éƒ¨ç–¾ç—…"],
                'ground_truth': "èƒ¸éƒ¨CT"
            },
            {
                'question': "45å²å¥³æ€§ï¼Œæ…¢æ€§åå¤å¤´ç—›3å¹´ï¼Œæ— ç¥ç»ç³»ç»Ÿå¼‚å¸¸ä½“å¾ã€‚",
                'answer': "å»ºè®®è¿›è¡ŒMRé¢…è„‘å¹³æ‰«æ£€æŸ¥ï¼Œä»¥æ’é™¤é¢…å†…å ä½æ€§ç—…å˜ã€‚",
                'contexts': ["æ…¢æ€§å¤´ç—›éœ€è¦å½±åƒå­¦æ£€æŸ¥", "MRIæ˜¯è¯Šæ–­é¢…å†…ç–¾ç—…çš„é¦–é€‰æ–¹æ³•"],
                'ground_truth': "MRé¢…è„‘(å¹³æ‰«)"
            }
        ]
        
        print(f"æµ‹è¯•æ ·æœ¬æ•°é‡: {len(test_samples)}")
        
        # è¿è¡Œè¯„ä¼°
        results = await evaluator.evaluate_batch_async(test_samples)
        
        print(f"\nâœ… è¯„ä¼°å®Œæˆ!")
        print(f"æ€»æ ·æœ¬æ•°: {results['total_samples']}")
        print(f"å¹³å‡è¯„åˆ†:")
        for metric, score in results['avg_scores'].items():
            print(f"  {metric}: {score:.4f}")
        
        # æ˜¾ç¤ºä¸ªåˆ«æ ·æœ¬ç»“æœ
        print(f"\nä¸ªåˆ«æ ·æœ¬ç»“æœ:")
        for i, scores in enumerate(results['individual_scores']):
            print(f"  æ ·æœ¬ {i+1}:")
            for metric, score in scores.items():
                print(f"    {metric}: {score:.4f}")
        
        return results
        
    except Exception as e:
        print(f"âŒ å®Œæ•´è¯„ä¼°æµ‹è¯•å¤±è´¥: {e}")
        logger.error(f"å®Œæ•´è¯„ä¼°æµ‹è¯•è¯¦ç»†é”™è¯¯: {e}", exc_info=True)
        return None

def test_sync_interface():
    """æµ‹è¯•åŒæ­¥æ¥å£"""
    print("\n=== æµ‹è¯•åŒæ­¥æ¥å£ ===")
    
    try:
        from app.services.ragas_evaluator_v2 import RAGASEvaluatorV2
        
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = RAGASEvaluatorV2()
        
        # æµ‹è¯•æ•°æ®
        sample_data = {
            'question': "æ‚£è€…å‡ºç°èƒ¸ç—›ç—‡çŠ¶ï¼Œéœ€è¦è¿›è¡Œä»€ä¹ˆå½±åƒå­¦æ£€æŸ¥ï¼Ÿ",
            'answer': "å»ºè®®è¿›è¡Œèƒ¸éƒ¨CTæ£€æŸ¥ã€‚",
            'contexts': ["èƒ¸ç—›æ˜¯å¸¸è§çš„ä¸´åºŠç—‡çŠ¶", "CTæ£€æŸ¥å¯ä»¥æœ‰æ•ˆè¯Šæ–­èƒ¸éƒ¨ç–¾ç—…"],
            'ground_truth': "èƒ¸éƒ¨CT"
        }
        
        # æµ‹è¯•åŒæ­¥å•æ ·æœ¬è¯„ä¼°
        scores = evaluator.evaluate_single_sample(sample_data)
        print(f"âœ… åŒæ­¥å•æ ·æœ¬è¯„ä¼°æˆåŠŸ:")
        for metric, score in scores.items():
            print(f"  {metric}: {score:.4f}")
        
        # æµ‹è¯•åŒæ­¥æ‰¹é‡è¯„ä¼°
        test_samples = [sample_data]
        batch_results = evaluator.evaluate_batch(test_samples)
        print(f"âœ… åŒæ­¥æ‰¹é‡è¯„ä¼°æˆåŠŸ:")
        print(f"  å¹³å‡è¯„åˆ†: {batch_results['avg_scores']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ åŒæ­¥æ¥å£æµ‹è¯•å¤±è´¥: {e}")
        logger.error(f"åŒæ­¥æ¥å£æµ‹è¯•è¯¦ç»†é”™è¯¯: {e}", exc_info=True)
        return False

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹RAGAS V2è¯„ä¼°å™¨æµ‹è¯•")
    
    # æµ‹è¯•1: å•æŒ‡æ ‡è¯„ä¼°
    success1 = await test_single_metric()
    
    # æµ‹è¯•2: å®Œæ•´è¯„ä¼°æµç¨‹
    success2 = await test_full_evaluation()
    
    # æµ‹è¯•3: åŒæ­¥æ¥å£
    success3 = test_sync_interface()
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
    print(f"  å•æŒ‡æ ‡è¯„ä¼°: {'âœ… é€šè¿‡' if success1 else 'âŒ å¤±è´¥'}")
    print(f"  å®Œæ•´è¯„ä¼°æµç¨‹: {'âœ… é€šè¿‡' if success2 else 'âŒ å¤±è´¥'}")
    print(f"  åŒæ­¥æ¥å£: {'âœ… é€šè¿‡' if success3 else 'âŒ å¤±è´¥'}")
    
    if all([success1, success2, success3]):
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼RAGAS V2è¯„ä¼°å™¨å·¥ä½œæ­£å¸¸")
    else:
        print(f"\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")

if __name__ == "__main__":
    asyncio.run(main())