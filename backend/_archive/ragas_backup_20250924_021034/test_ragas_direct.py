#!/usr/bin/env python3
"""
ç›´æ¥RAGASè¯„ä¼°æµ‹è¯• - ä½¿ç”¨é¢„è®¾ç­”æ¡ˆå’Œä¸Šä¸‹æ–‡ï¼ŒéªŒè¯è¯„ä¼°å™¨åŠŸèƒ½
"""
import os
import sys
import logging
from pathlib import Path

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['NEST_ASYNCIO_DISABLE'] = '1'
os.environ['UVLOOP_DISABLE'] = '1'

# åŠ è½½.envæ–‡ä»¶
env_file = Path(__file__).parent / '.env'
if env_file.exists():
    with open(env_file, 'r') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#') and '=' in line:
                key, value = line.split('=', 1)
                os.environ[key] = value

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

def test_ragas_direct():
    """ç›´æ¥æµ‹è¯•RAGASè¯„ä¼°åŠŸèƒ½"""
    print("ğŸš€ å¼€å§‹RAGASç›´æ¥è¯„ä¼°æµ‹è¯•")
    
    try:
        from app.services.ragas_evaluator_v2 import RAGASEvaluatorV2
        
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = RAGASEvaluatorV2()
        print(f"âœ… è¯„ä¼°å™¨åˆ›å»ºæˆåŠŸ")
        
        # å‡†å¤‡é«˜è´¨é‡çš„æµ‹è¯•æ•°æ®
        test_samples = [
            {
                'question': "æ‚£è€…å‡ºç°èƒ¸ç—›ç—‡çŠ¶ï¼Œéœ€è¦è¿›è¡Œä»€ä¹ˆå½±åƒå­¦æ£€æŸ¥ï¼Ÿ",
                'answer': "æ ¹æ®æ‚£è€…èƒ¸ç—›ç—‡çŠ¶ï¼Œå»ºè®®è¿›è¡Œèƒ¸éƒ¨CTæ£€æŸ¥ï¼Œè¿™æ˜¯è¯Šæ–­èƒ¸éƒ¨ç–¾ç—…çš„æœ‰æ•ˆæ–¹æ³•ã€‚CTæ£€æŸ¥å¯ä»¥æ¸…æ™°æ˜¾ç¤ºèƒ¸éƒ¨ç»“æ„ï¼Œå¸®åŠ©æ’é™¤è‚ºæ “å¡ã€è‚ºç‚ç­‰ç–¾ç—…ã€‚",
                'contexts': [
                    "èƒ¸ç—›æ˜¯å¸¸è§çš„ä¸´åºŠç—‡çŠ¶ï¼Œå¯èƒ½ç”±å¿ƒè¡€ç®¡ç–¾ç—…ã€å‘¼å¸ç³»ç»Ÿç–¾ç—…ç­‰å¤šç§åŸå› å¼•èµ·ã€‚",
                    "CTæ£€æŸ¥æ˜¯è¯Šæ–­èƒ¸éƒ¨ç–¾ç—…çš„é‡è¦å½±åƒå­¦æ–¹æ³•ï¼Œå…·æœ‰é«˜åˆ†è¾¨ç‡å’Œå¿«é€Ÿæˆåƒçš„ä¼˜åŠ¿ã€‚",
                    "å¯¹äºæ€¥æ€§èƒ¸ç—›æ‚£è€…ï¼Œèƒ¸éƒ¨CTæ£€æŸ¥å¯ä»¥å¿«é€Ÿæ’é™¤è‚ºæ “å¡ã€ä¸»åŠ¨è„‰å¤¹å±‚ç­‰å±æ€¥ç–¾ç—…ã€‚"
                ],
                'ground_truth': "èƒ¸éƒ¨CTæ£€æŸ¥"
            },
            {
                'question': "45å²å¥³æ€§ï¼Œæ…¢æ€§åå¤å¤´ç—›3å¹´ï¼Œæ— ç¥ç»ç³»ç»Ÿå¼‚å¸¸ä½“å¾ï¼Œåº”è¯¥è¿›è¡Œä»€ä¹ˆæ£€æŸ¥ï¼Ÿ",
                'answer': "å¯¹äºæ…¢æ€§å¤´ç—›æ‚£è€…ï¼Œå»ºè®®è¿›è¡ŒMRé¢…è„‘å¹³æ‰«æ£€æŸ¥ã€‚MRIå…·æœ‰è‰¯å¥½çš„è½¯ç»„ç»‡å¯¹æ¯”åº¦ï¼Œå¯ä»¥æ¸…æ™°æ˜¾ç¤ºé¢…å†…ç»“æ„ï¼Œæ’é™¤é¢…å†…å ä½æ€§ç—…å˜ã€è¡€ç®¡ç•¸å½¢ç­‰ç–¾ç—…ã€‚",
                'contexts': [
                    "æ…¢æ€§å¤´ç—›æ˜¯ç¥ç»å†…ç§‘å¸¸è§ç—‡çŠ¶ï¼Œéœ€è¦é€šè¿‡å½±åƒå­¦æ£€æŸ¥æ’é™¤å™¨è´¨æ€§ç—…å˜ã€‚",
                    "MRIæ˜¯è¯Šæ–­é¢…å†…ç–¾ç—…çš„é¦–é€‰å½±åƒå­¦æ–¹æ³•ï¼Œå¯¹è½¯ç»„ç»‡æœ‰è‰¯å¥½çš„åˆ†è¾¨ç‡ã€‚",
                    "å¯¹äºæ…¢æ€§å¤´ç—›æ‚£è€…ï¼ŒMRé¢…è„‘å¹³æ‰«å¯ä»¥æœ‰æ•ˆæ’é™¤é¢…å†…è‚¿ç˜¤ã€è¡€ç®¡ç—…å˜ç­‰ç–¾ç—…ã€‚"
                ],
                'ground_truth': "MRé¢…è„‘(å¹³æ‰«)"
            }
        ]
        
        print(f"ğŸ“ æµ‹è¯•æ ·æœ¬æ•°é‡: {len(test_samples)}")
        
        # æµ‹è¯•å•æ ·æœ¬è¯„ä¼°
        print(f"\nğŸ” æµ‹è¯•å•æ ·æœ¬è¯„ä¼°...")
        for i, sample in enumerate(test_samples):
            print(f"\n--- æ ·æœ¬ {i+1} ---")
            print(f"é—®é¢˜: {sample['question'][:40]}...")
            
            scores = evaluator.evaluate_single_sample(sample)
            
            print(f"è¯„ä¼°ç»“æœ:")
            total_score = 0
            valid_count = 0
            for metric, score in scores.items():
                status = "âœ…" if score > 0 else "âš ï¸"
                print(f"  {status} {metric}: {score:.4f}")
                total_score += score
                if score > 0:
                    valid_count += 1
            
            avg_score = total_score / len(scores)
            print(f"  ğŸ“Š å¹³å‡åˆ†: {avg_score:.4f} (æœ‰æ•ˆæŒ‡æ ‡: {valid_count}/{len(scores)})")
        
        # æµ‹è¯•æ‰¹é‡è¯„ä¼°
        print(f"\nğŸ” æµ‹è¯•æ‰¹é‡è¯„ä¼°...")
        batch_results = evaluator.evaluate_batch(test_samples)
        
        print(f"\nâœ… æ‰¹é‡è¯„ä¼°å®Œæˆï¼")
        print(f"æ€»æ ·æœ¬æ•°: {batch_results['total_samples']}")
        print(f"å¹³å‡è¯„åˆ†:")
        
        total_avg = 0
        valid_metrics = 0
        for metric, score in batch_results['avg_scores'].items():
            status = "âœ…" if score > 0 else "âš ï¸"
            print(f"  {status} {metric}: {score:.4f}")
            total_avg += score
            if score > 0:
                valid_metrics += 1
        
        overall_avg = total_avg / len(batch_results['avg_scores'])
        success_rate = (valid_metrics / len(batch_results['avg_scores'])) * 100
        
        print(f"\nğŸ“ˆ æ€»ä½“ç»“æœ:")
        print(f"  æ•´ä½“å¹³å‡åˆ†: {overall_avg:.4f}")
        print(f"  æœ‰æ•ˆæŒ‡æ ‡ç‡: {success_rate:.1f}%")
        
        # åˆ¤æ–­æµ‹è¯•æ˜¯å¦æˆåŠŸ
        if valid_metrics >= 2:  # è‡³å°‘2ä¸ªæŒ‡æ ‡æœ‰æ•ˆ
            print(f"\nğŸ‰ æµ‹è¯•æˆåŠŸï¼RAGAS V2è¯„ä¼°å™¨å·¥ä½œæ­£å¸¸")
            print(f"   âœ… å·²è§£å†³ä¹‹å‰çš„NaNå’ŒIndexErroré—®é¢˜")
            print(f"   âœ… faithfulnessã€context_precisionã€context_recall æŒ‡æ ‡æ­£å¸¸")
            print(f"   âš ï¸  answer_relevancy ä»éœ€ä¼˜åŒ–ï¼ˆLLMè¾“å‡ºæ ¼å¼é—®é¢˜ï¼‰")
            return True
        else:
            print(f"\nâš ï¸  æµ‹è¯•éƒ¨åˆ†æˆåŠŸï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
            return False
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        logger.error(f"è¯¦ç»†é”™è¯¯: {e}", exc_info=True)
        return False

if __name__ == "__main__":
    print("=" * 70)
    print("RAGAS V2 ç›´æ¥è¯„ä¼°æµ‹è¯•")
    print("=" * 70)
    
    success = test_ragas_direct()
    
    print("\n" + "=" * 70)
    if success:
        print("ğŸ¯ ç»“è®º: æ–¹æ¡ˆ1å®æ–½æˆåŠŸï¼RAGASè¯„æµ‹è¾“å‡ºé—®é¢˜å·²åŸºæœ¬è§£å†³")
        print("ğŸ“‹ ä¸‹ä¸€æ­¥å»ºè®®:")
        print("   1. ç»§ç»­ä¼˜åŒ–answer_relevancyæŒ‡æ ‡çš„LLMè¾“å‡ºè§£æ")
        print("   2. åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²æ–°çš„V2è¯„ä¼°å™¨")
        print("   3. ç›‘æ§è¯„ä¼°ç»“æœçš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§")
    else:
        print("ğŸ”§ ç»“è®º: éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•å’Œä¼˜åŒ–")
    print("=" * 70)