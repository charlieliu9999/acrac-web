#!/usr/bin/env python3
"""
ä½¿ç”¨çœŸå®RAG+LLMæ¨ç†APIæ•°æ®è¿›è¡ŒRAGASè¯„æµ‹
"""

import os
import sys
import json
import requests
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(f'real_rag_evaluation_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    ]
)
logger = logging.getLogger(__name__)

# APIé…ç½®
API_BASE = "http://127.0.0.1:8000/api/v1"
RAG_API_URL = f"{API_BASE}/acrac/rag-llm/intelligent-recommendation"

def load_environment():
    """åŠ è½½ç¯å¢ƒå˜é‡"""
    env_file = project_root / ".env"
    if env_file.exists():
        with open(env_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key] = value
        logger.info("âœ… ç¯å¢ƒå˜é‡åŠ è½½å®Œæˆ")
    else:
        logger.warning("âš ï¸ .envæ–‡ä»¶ä¸å­˜åœ¨")

def call_real_rag_api(clinical_query: str, ground_truth: str = "") -> Dict[str, Any]:
    """è°ƒç”¨çœŸå®çš„RAG+LLMæ¨ç†API"""
    try:
        payload = {
            "clinical_query": clinical_query,
            "include_raw_data": True,
            "debug_mode": True,
            "top_scenarios": 5,
            "top_recommendations_per_scenario": 3,
            "show_reasoning": True,
            "similarity_threshold": 0.6,
            "compute_ragas": False,  # æˆ‘ä»¬è‡ªå·±è®¡ç®—RAGAS
            "ground_truth": ground_truth
        }
        
        logger.info(f"è°ƒç”¨RAG API: {clinical_query[:50]}...")
        response = requests.post(RAG_API_URL, json=payload, timeout=120)
        response.raise_for_status()
        
        result = response.json()
        if result.get("success"):
            logger.info("âœ… RAG APIè°ƒç”¨æˆåŠŸ")
            return result
        else:
            logger.error(f"âŒ RAG APIè°ƒç”¨å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
            return {}
            
    except Exception as e:
        logger.error(f"âŒ RAG APIè°ƒç”¨å¼‚å¸¸: {e}")
        return {}

def extract_contexts_from_trace(trace_data: Dict[str, Any]) -> List[str]:
    """ä»traceæ•°æ®ä¸­æå–ä¸Šä¸‹æ–‡"""
    contexts = []
    
    # ä»scenariosä¸­æå–
    scenarios = trace_data.get("scenarios", [])
    for scenario in scenarios:
        if isinstance(scenario, dict):
            desc = scenario.get("description_zh", "")
            if desc:
                contexts.append(desc)
    
    # ä»scenarios_with_recommendationsä¸­æå–
    scenarios_with_recs = trace_data.get("scenarios_with_recommendations", [])
    for scenario in scenarios_with_recs:
        if isinstance(scenario, dict):
            desc = scenario.get("description_zh", "")
            if desc and desc not in contexts:
                contexts.append(desc)
    
    return contexts

def convert_to_ragas_format(rag_result: Dict[str, Any], clinical_query: str, ground_truth: str) -> Dict[str, Any]:
    """å°†RAG APIç»“æœè½¬æ¢ä¸ºRAGASè¯„æµ‹æ ¼å¼"""
    
    # æå–LLMæ¨èç»“æœä½œä¸ºç­”æ¡ˆ
    llm_recommendations = rag_result.get("llm_recommendations", {})
    recommendations = llm_recommendations.get("recommendations", [])
    
    # æ„é€ ç­”æ¡ˆæ–‡æœ¬
    answer_parts = []
    if llm_recommendations.get("summary"):
        answer_parts.append(f"æ¨èæ€»ç»“: {llm_recommendations['summary']}")
    
    for i, rec in enumerate(recommendations[:3], 1):
        rec_text = f"{i}. {rec.get('procedure_name', 'æœªçŸ¥æ£€æŸ¥')} (è¯„åˆ†: {rec.get('appropriateness_rating', 'N/A')})"
        if rec.get('recommendation_reason'):
            rec_text += f" - {rec['recommendation_reason']}"
        answer_parts.append(rec_text)
    
    answer = "\n".join(answer_parts) if answer_parts else json.dumps(llm_recommendations, ensure_ascii=False)
    
    # æå–ä¸Šä¸‹æ–‡
    contexts = []
    
    # ä»traceä¸­æå–
    trace_data = rag_result.get("trace", {})
    if trace_data:
        contexts.extend(extract_contexts_from_trace(trace_data))
    
    # ä»scenariosä¸­æå–
    scenarios = rag_result.get("scenarios", [])
    for scenario in scenarios:
        if isinstance(scenario, dict):
            desc = scenario.get("description_zh", "")
            if desc and desc not in contexts:
                contexts.append(desc)
    
    # å¦‚æœæ²¡æœ‰ä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨é»˜è®¤å€¼
    if not contexts:
        contexts = ["æ— å¯ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯"]
    
    return {
        "question": clinical_query,
        "answer": answer,
        "contexts": contexts,
        "ground_truth": ground_truth
    }

def run_ragas_evaluation(ragas_data: Dict[str, Any]) -> Dict[str, float]:
    """è¿è¡ŒRAGASè¯„æµ‹"""
    try:
        # ä½¿ç”¨é¡¹ç›®ä¸­å·²æœ‰çš„RAGASè¯„ä¼°å™¨
        from app.services.ragas_evaluator import RAGASEvaluator
        
        logger.info("åˆå§‹åŒ–RAGASè¯„ä¼°å™¨...")
        evaluator = RAGASEvaluator()
        
        # åˆ›å»ºæµ‹è¯•æ ·æœ¬
        test_sample = evaluator.create_test_sample(
            question=ragas_data["question"],
            answer=ragas_data["answer"],
            contexts=ragas_data["contexts"],
            ground_truth=ragas_data["ground_truth"]
        )
        
        # è¿è¡Œå•æ ·æœ¬è¯„æµ‹
        logger.info("å¼€å§‹RAGASè¯„æµ‹...")
        scores = evaluator.evaluate_single_sample(test_sample)
        
        logger.info("âœ… RAGASè¯„æµ‹å®Œæˆ")
        return scores
        
    except ImportError as e:
        logger.warning(f"âš ï¸ RAGASåº“æœªå®‰è£…æˆ–é…ç½®ä¸æ­£ç¡®: {e}")
        return {
            "faithfulness": 0.0,
            "answer_relevancy": 0.0,
            "context_precision": 0.0,
            "context_recall": 0.0
        }
    except Exception as e:
        logger.error(f"âŒ RAGASè¯„æµ‹å¤±è´¥: {e}")
        return {
            "faithfulness": 0.0,
            "answer_relevancy": 0.0,
            "context_precision": 0.0,
            "context_recall": 0.0
        }

def save_evaluation_result(result: Dict[str, Any], filename: str = None):
    """ä¿å­˜è¯„æµ‹ç»“æœ"""
    if not filename:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"real_rag_evaluation_result_{timestamp}.json"
    
    output_path = project_root / "uploads" / "ragas" / filename
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    logger.info(f"âœ… è¯„æµ‹ç»“æœå·²ä¿å­˜: {output_path}")
    return output_path

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹çœŸå®RAG+LLMæ•°æ®çš„RAGASè¯„æµ‹")
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_environment()
    
    # ä½¿ç”¨ç”¨æˆ·æä¾›çš„çœŸå®æ•°æ®ç¤ºä¾‹
    test_case = {
        "clinical_query": "50å²å¥³æ€§ï¼Œ3å¤©å‰å¼€å§‹å‡ºç°å·¦ä¾§è‚¢ä½“æ— åŠ›ï¼Œä¼´æ–°å‘å¤´ç—›ã€‚",
        "ground_truth": "MRé¢…è„‘(å¹³æ‰«+å¢å¼º)"
    }
    
    logger.info(f"æµ‹è¯•ç”¨ä¾‹: {test_case['clinical_query']}")
    logger.info(f"æ ‡å‡†ç­”æ¡ˆ: {test_case['ground_truth']}")
    
    # 1. è°ƒç”¨çœŸå®RAG API
    logger.info("ğŸ“¡ è°ƒç”¨çœŸå®RAG+LLMæ¨ç†API...")
    rag_result = call_real_rag_api(
        clinical_query=test_case["clinical_query"],
        ground_truth=test_case["ground_truth"]
    )
    
    if not rag_result:
        logger.error("âŒ æ— æ³•è·å–RAGæ¨ç†ç»“æœï¼Œé€€å‡º")
        return
    
    # 2. è½¬æ¢ä¸ºRAGASæ ¼å¼
    logger.info("ğŸ”„ è½¬æ¢æ•°æ®æ ¼å¼...")
    ragas_data = convert_to_ragas_format(
        rag_result=rag_result,
        clinical_query=test_case["clinical_query"],
        ground_truth=test_case["ground_truth"]
    )
    
    logger.info(f"è½¬æ¢åçš„æ•°æ®:")
    logger.info(f"  é—®é¢˜: {ragas_data['question'][:100]}...")
    logger.info(f"  ç­”æ¡ˆ: {ragas_data['answer'][:100]}...")
    logger.info(f"  ä¸Šä¸‹æ–‡æ•°é‡: {len(ragas_data['contexts'])}")
    logger.info(f"  æ ‡å‡†ç­”æ¡ˆ: {ragas_data['ground_truth']}")
    
    # 3. è¿è¡ŒRAGASè¯„æµ‹
    logger.info("ğŸ“Š è¿è¡ŒRAGASè¯„æµ‹...")
    ragas_scores = run_ragas_evaluation(ragas_data)
    
    # 4. æ„é€ å®Œæ•´ç»“æœ
    evaluation_result = {
        "timestamp": datetime.now().isoformat(),
        "test_case": test_case,
        "rag_api_result": {
            "success": rag_result.get("success"),
            "processing_time_ms": rag_result.get("processing_time_ms"),
            "model_used": rag_result.get("model_used"),
            "similarity_threshold": rag_result.get("similarity_threshold"),
            "max_similarity": rag_result.get("max_similarity"),
            "is_low_similarity_mode": rag_result.get("is_low_similarity_mode"),
            "llm_recommendations": rag_result.get("llm_recommendations"),
            "scenarios_count": len(rag_result.get("scenarios", [])),
        },
        "ragas_data": ragas_data,
        "ragas_scores": ragas_scores,
        "evaluation_summary": {
            "avg_score": sum(ragas_scores.values()) / len(ragas_scores) if ragas_scores else 0,
            "best_metric": max(ragas_scores.items(), key=lambda x: x[1]) if ragas_scores else ("none", 0),
            "worst_metric": min(ragas_scores.items(), key=lambda x: x[1]) if ragas_scores else ("none", 0),
        }
    }
    
    # 5. ä¿å­˜ç»“æœ
    output_path = save_evaluation_result(evaluation_result)
    
    # 6. è¾“å‡ºæ€»ç»“
    logger.info("ğŸ“‹ è¯„æµ‹ç»“æœæ€»ç»“:")
    logger.info(f"  Faithfulness: {ragas_scores.get('faithfulness', 0):.3f}")
    logger.info(f"  Answer Relevancy: {ragas_scores.get('answer_relevancy', 0):.3f}")
    logger.info(f"  Context Precision: {ragas_scores.get('context_precision', 0):.3f}")
    logger.info(f"  Context Recall: {ragas_scores.get('context_recall', 0):.3f}")
    logger.info(f"  å¹³å‡åˆ†: {evaluation_result['evaluation_summary']['avg_score']:.3f}")
    
    logger.info("ğŸ‰ çœŸå®RAG+LLMæ•°æ®çš„RAGASè¯„æµ‹å®Œæˆ!")
    return output_path

if __name__ == "__main__":
    main()