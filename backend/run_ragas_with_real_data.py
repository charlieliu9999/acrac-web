#!/usr/bin/env python3
"""
ä½¿ç”¨çœŸå®æ¨ç†æ•°æ®è¿è¡ŒRAGASè¯„æµ‹
"""

import os
import json
import pandas as pd
from datetime import datetime
from datasets import Dataset

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["NUMEXPR_MAX_THREADS"] = "8"

# RAGASç›¸å…³å¯¼å…¥
from ragas import evaluate
from ragas.metrics import (
    answer_relevancy,
    context_precision,
    context_recall,
    faithfulness,
    answer_correctness,
    answer_similarity
)

# LangChainç›¸å…³å¯¼å…¥
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

def setup_models():
    """è®¾ç½®è¯„æµ‹æ¨¡å‹"""
    print("=== è®¾ç½®è¯„æµ‹æ¨¡å‹ ===")
    
    # SiliconFlow APIé…ç½®
    api_key = os.getenv("SILICONFLOW_API_KEY")
    base_url = os.getenv("SILICONFLOW_BASE_URL", "https://api.siliconflow.cn/v1")
    llm_model = os.getenv("SILICONFLOW_LLM_MODEL", "Qwen/Qwen2.5-32B-Instruct")
    embedding_model = os.getenv("SILICONFLOW_EMBEDDING_MODEL", "BAAI/bge-m3")
    
    if not api_key:
        raise ValueError("è¯·è®¾ç½® SILICONFLOW_API_KEY ç¯å¢ƒå˜é‡")
    
    print(f"LLMæ¨¡å‹: {llm_model}")
    print(f"åµŒå…¥æ¨¡å‹: {embedding_model}")
    
    # åˆå§‹åŒ–æ¨¡å‹
    llm = ChatOpenAI(
        model=llm_model,
        api_key=api_key,
        base_url=base_url,
        temperature=0.1
    )
    
    embeddings = OpenAIEmbeddings(
        model=embedding_model,
        api_key=api_key,
        base_url=base_url
    )
    
    return llm, embeddings

def load_real_data(filename):
    """åŠ è½½çœŸå®æ¨ç†æ•°æ®"""
    print(f"=== åŠ è½½çœŸå®æ•°æ®: {filename} ===")
    
    if not os.path.exists(filename):
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
    
    with open(filename, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"åŠ è½½äº† {len(data)} æ¡æ•°æ®")
    
    # è½¬æ¢ä¸ºRAGASæ ¼å¼
    ragas_data = {
        "question": [],
        "answer": [],
        "contexts": [],
        "ground_truth": []
    }
    
    for item in data:
        ragas_data["question"].append(item["question"])
        ragas_data["answer"].append(item["answer"])
        ragas_data["contexts"].append(item["contexts"])
        # ç”±äºæ²¡æœ‰æ ‡å‡†ç­”æ¡ˆï¼Œä½¿ç”¨answerä½œä¸ºground_truth
        ragas_data["ground_truth"].append(item["answer"])
    
    return ragas_data

def run_ragas_evaluation(data, llm, embeddings):
    """è¿è¡ŒRAGASè¯„æµ‹"""
    print("=== å¼€å§‹RAGASè¯„æµ‹ ===")
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = Dataset.from_dict(data)
    print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")
    
    # é€‰æ‹©è¯„æµ‹æŒ‡æ ‡
    metrics = [
        answer_relevancy,
        context_precision,
        context_recall,
        faithfulness
    ]
    
    # åªæœ‰å½“æœ‰ground_truthæ—¶æ‰ä½¿ç”¨è¿™äº›æŒ‡æ ‡
    if any(gt for gt in data["ground_truth"]):
        metrics.extend([
            answer_correctness,
            answer_similarity
        ])
    
    print(f"ä½¿ç”¨çš„è¯„æµ‹æŒ‡æ ‡: {[metric.name for metric in metrics]}")
    
    try:
        # è¿è¡Œè¯„æµ‹
        result = evaluate(
            dataset=dataset,
            metrics=metrics,
            llm=llm,
            embeddings=embeddings
        )
        
        print("âœ… RAGASè¯„æµ‹å®Œæˆ")
        return result
        
    except Exception as e:
        print(f"âŒ RAGASè¯„æµ‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def analyze_results(result, data):
    """åˆ†æè¯„æµ‹ç»“æœ"""
    if not result:
        print("âŒ æ²¡æœ‰è¯„æµ‹ç»“æœå¯åˆ†æ")
        return
    
    print("\n=== è¯„æµ‹ç»“æœåˆ†æ ===")
    
    # è½¬æ¢ä¸ºDataFrameä¾¿äºåˆ†æ
    df = result.to_pandas()
    
    print(f"è¯„æµ‹æ•°æ®æ¡æ•°: {len(df)}")
    print(f"è¯„æµ‹æŒ‡æ ‡: {list(df.columns)}")
    
    # è®¡ç®—å„æŒ‡æ ‡çš„ç»Ÿè®¡ä¿¡æ¯
    numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns
    
    print("\nğŸ“Š æŒ‡æ ‡ç»Ÿè®¡:")
    for col in numeric_columns:
        if col not in ['question', 'answer', 'contexts', 'ground_truth']:
            values = df[col].dropna()
            if len(values) > 0:
                print(f"  {col}:")
                print(f"    å¹³å‡å€¼: {values.mean():.4f}")
                print(f"    ä¸­ä½æ•°: {values.median():.4f}")
                print(f"    æ ‡å‡†å·®: {values.std():.4f}")
                print(f"    æœ€å°å€¼: {values.min():.4f}")
                print(f"    æœ€å¤§å€¼: {values.max():.4f}")
                print(f"    NaNæ•°é‡: {df[col].isna().sum()}")
    
    # æ£€æŸ¥NaNå€¼
    print("\nğŸ” NaNå€¼æ£€æŸ¥:")
    for col in df.columns:
        nan_count = df[col].isna().sum()
        if nan_count > 0:
            print(f"  {col}: {nan_count} ä¸ªNaNå€¼ ({nan_count/len(df)*100:.1f}%)")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_filename = f"ragas_real_data_results_{timestamp}.csv"
    df.to_csv(result_filename, index=False, encoding='utf-8')
    print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {result_filename}")
    
    # æ˜¾ç¤ºå…·ä½“æ¡ˆä¾‹
    print("\nğŸ“‹ å…·ä½“æ¡ˆä¾‹åˆ†æ:")
    for i, row in df.head(3).iterrows():
        print(f"\næ¡ˆä¾‹ {i+1}:")
        print(f"  é—®é¢˜: {row['question'][:100]}...")
        print(f"  ç­”æ¡ˆ: {row['answer'][:100]}...")
        print(f"  ä¸Šä¸‹æ–‡æ•°é‡: {len(row['contexts']) if isinstance(row['contexts'], list) else 'N/A'}")
        
        for col in numeric_columns:
            if col not in ['question', 'answer', 'contexts', 'ground_truth']:
                value = row[col]
                if pd.notna(value):
                    print(f"  {col}: {value:.4f}")
                else:
                    print(f"  {col}: NaN")

def main():
    """ä¸»å‡½æ•°"""
    try:
        # æŸ¥æ‰¾æœ€æ–°çš„æ•°æ®æ–‡ä»¶
        data_files = [f for f in os.listdir('.') if f.startswith('correct_ragas_data_') and f.endswith('.json')]
        if not data_files:
            print("âŒ æœªæ‰¾åˆ°çœŸå®æ¨ç†æ•°æ®æ–‡ä»¶")
            return
        
        # ä½¿ç”¨æœ€æ–°çš„æ–‡ä»¶
        latest_file = sorted(data_files)[-1]
        print(f"ä½¿ç”¨æ•°æ®æ–‡ä»¶: {latest_file}")
        
        # è®¾ç½®æ¨¡å‹
        llm, embeddings = setup_models()
        
        # åŠ è½½æ•°æ®
        data = load_real_data(latest_file)
        
        # è¿è¡Œè¯„æµ‹
        result = run_ragas_evaluation(data, llm, embeddings)
        
        # åˆ†æç»“æœ
        analyze_results(result, data)
        
        print("\nğŸ‰ çœŸå®æ•°æ®RAGASè¯„æµ‹å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ è¯„æµ‹è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()