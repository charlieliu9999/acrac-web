#!/usr/bin/env python3
"""
çœŸå®æ•°æ®è¯„æµ‹è„šæœ¬
ä½¿ç”¨å¢å¼ºç‰ˆRAGASè¯„ä¼°å™¨å¯¹çœŸå®æ¨ç†æ•°æ®è¿›è¡Œå®Œæ•´è¯„æµ‹
"""
import os
import sys
import json
import logging
from pathlib import Path
from datetime import datetime

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['NEST_ASYNCIO_DISABLE'] = '1'
os.environ['UVLOOP_DISABLE'] = '1'

# åŠ è½½.envæ–‡ä»¶
env_file = Path(__file__).parent / '.env'
if env_file.exists():
    with open(env_file, 'r') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#') and '=' in line:
                key, value = line.split('=', 1)
                os.environ[key] = value

sys.path.insert(0, str(Path(__file__).parent))

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

def extract_answer_from_result(result_data, inference_method):
    """ä»resultæ•°æ®ä¸­æå–ç­”æ¡ˆ"""
    if not result_data or not isinstance(result_data, dict):
        return None
    
    # RAGæ–¹å¼ï¼šç›´æ¥ä»answerå­—æ®µè·å–
    if inference_method == 'rag' and 'answer' in result_data:
        answer = result_data['answer']
        if isinstance(answer, str) and answer.strip():
            return answer.strip()
    
    # æ–°æ ¼å¼ï¼šä»llm_recommendationsè§£æ
    if 'llm_recommendations' in result_data:
        llm_rec = result_data['llm_recommendations']
        if isinstance(llm_rec, dict) and 'recommendations' in llm_rec:
            recommendations = llm_rec['recommendations']
            if isinstance(recommendations, list) and recommendations:
                # å°†æ¨èç»“æœè½¬æ¢ä¸ºæ–‡æœ¬
                rec_texts = []
                for rec in recommendations:
                    if isinstance(rec, dict) and 'procedure_name' in rec:
                        rank = rec.get('rank', '')
                        procedure = rec.get('procedure_name', '')
                        rating = rec.get('appropriateness_rating', '')
                        reason = rec.get('recommendation_reason', '')
                        
                        rec_text = f"æ¨è{rank}: {procedure}"
                        if rating:
                            rec_text += f" (é€‚å®œæ€§: {rating})"
                        if reason:
                            rec_text += f" - {reason[:100]}..."  # é™åˆ¶é•¿åº¦
                        rec_texts.append(rec_text)
                
                if rec_texts:
                    return "\\n".join(rec_texts)
    
    return None

def extract_contexts_from_result(result_data, inference_method):
    """ä»resultæ•°æ®ä¸­æå–ä¸Šä¸‹æ–‡"""
    contexts = []
    
    if not result_data or not isinstance(result_data, dict):
        return contexts
    
    # ç›´æ¥ä»contextså­—æ®µè·å–
    if 'contexts' in result_data:
        contexts_data = result_data['contexts']
        if isinstance(contexts_data, list):
            for ctx in contexts_data:
                if isinstance(ctx, str) and ctx.strip():
                    contexts.append(ctx.strip())
                elif isinstance(ctx, dict):
                    # å°è¯•ä»å­—å…¸ä¸­æå–æ–‡æœ¬
                    for key in ['content', 'text', 'description']:
                        if key in ctx and isinstance(ctx[key], str) and ctx[key].strip():
                            contexts.append(ctx[key].strip())
                            break
    
    # ä»scenariosè·å–ä¸Šä¸‹æ–‡
    if 'scenarios' in result_data:
        scenarios = result_data['scenarios']
        if isinstance(scenarios, list):
            for scenario in scenarios:
                if isinstance(scenario, dict):
                    # æ„å»ºåœºæ™¯æè¿°ä½œä¸ºä¸Šä¸‹æ–‡
                    context_parts = []
                    if 'description_zh' in scenario:
                        context_parts.append(f"åœºæ™¯: {scenario['description_zh']}")
                    if 'clinical_context' in scenario:
                        context_parts.append(f"ä¸´åºŠèƒŒæ™¯: {scenario['clinical_context']}")
                    if 'patient_population' in scenario:
                        context_parts.append(f"æ‚£è€…ç¾¤ä½“: {scenario['patient_population']}")
                    
                    if context_parts:
                        contexts.append(" | ".join(context_parts))
    
    return contexts

def load_real_data():
    """åŠ è½½çœŸå®æ¨ç†æ•°æ®"""
    try:
        from app.core.database import get_db
        from app.models.system_models import InferenceLog
        
        # è·å–æ•°æ®åº“ä¼šè¯
        db = next(get_db())
        
        # æŸ¥è¯¢æŒ‡å®šIDçš„æ¨ç†è®°å½•
        run_ids = [42, 43, 44]
        runs = db.query(InferenceLog).filter(InferenceLog.id.in_(run_ids)).all()
        
        print(f"ğŸ“Š ä»æ•°æ®åº“åŠ è½½äº† {len(runs)} æ¡æ¨ç†è®°å½•")
        
        ragas_data_list = []
        
        for run in runs:
            print(f"\\nğŸ” å¤„ç†è®°å½• ID: {run.id}")
            print(f"   æŸ¥è¯¢: {run.query_text}")
            
            if not run.result:
                print("   âŒ æ— ç»“æœæ•°æ®ï¼Œè·³è¿‡")
                continue
            
            result = run.result if isinstance(run.result, dict) else json.loads(run.result)
            
            # æå–ç­”æ¡ˆ
            answer = extract_answer_from_result(result, run.inference_method)
            if not answer:
                print("   âŒ æ— æ³•æå–ç­”æ¡ˆï¼Œè·³è¿‡")
                continue
            
            # æå–ä¸Šä¸‹æ–‡
            contexts = extract_contexts_from_result(result, run.inference_method)
            if not contexts:
                print("   âŒ æ— æ³•æå–ä¸Šä¸‹æ–‡ï¼Œè·³è¿‡")
                continue
            
            # æ„å»ºRAGASæ•°æ®
            ragas_data = {
                "question": run.query_text,
                "answer": answer,
                "contexts": contexts,
                "ground_truth": "",  # æš‚æ—¶ä¸ºç©º
                "run_id": run.id,
                "inference_method": run.inference_method
            }
            
            ragas_data_list.append(ragas_data)
            print(f"   âœ… æ•°æ®æå–æˆåŠŸ")
            print(f"      - é—®é¢˜: {len(ragas_data['question'])} å­—ç¬¦")
            print(f"      - ç­”æ¡ˆ: {len(ragas_data['answer'])} å­—ç¬¦")
            print(f"      - ä¸Šä¸‹æ–‡: {len(ragas_data['contexts'])} ä¸ª")
        
        db.close()
        return ragas_data_list
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        logger.error(f"è¯¦ç»†é”™è¯¯: {e}", exc_info=True)
        return []

def run_evaluation():
    """è¿è¡Œå®Œæ•´è¯„æµ‹"""
    print("=" * 70)
    print("çœŸå®æ•°æ®RAGASè¯„æµ‹")
    print("=" * 70)
    
    # 1. åŠ è½½çœŸå®æ•°æ®
    real_data = load_real_data()
    if not real_data:
        print("âŒ æ— çœŸå®æ•°æ®å¯è¯„æµ‹")
        return
    
    # 2. åˆ›å»ºå¢å¼ºç‰ˆè¯„ä¼°å™¨
    try:
        from enhanced_ragas_evaluator import EnhancedRAGASEvaluator
        evaluator = EnhancedRAGASEvaluator()
        print(f"\\nâœ… å¢å¼ºç‰ˆè¯„ä¼°å™¨åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ è¯„ä¼°å™¨åˆ›å»ºå¤±è´¥: {e}")
        return
    
    # 3. è¿è¡Œæ‰¹é‡è¯„æµ‹
    print(f"\\nğŸš€ å¼€å§‹è¯„æµ‹ {len(real_data)} æ¡çœŸå®æ•°æ®")
    
    results = evaluator.evaluate_batch(real_data)
    
    # 4. æ˜¾ç¤ºè¯¦ç»†ç»“æœ
    print(f"\\nğŸ“Š è¯„æµ‹ç»“æœè¯¦æƒ…:")
    print(f"{'ID':<4} {'ä¸´åºŠåœºæ™¯':<30} {'Faithfulness':<12} {'Answer Relevancy':<15} {'Context Precision':<16} {'Context Recall':<13}")
    print("-" * 100)
    
    for i, (data, scores) in enumerate(zip(real_data, results['individual_scores'])):
        run_id = data['run_id']
        question = data['question'][:25] + "..." if len(data['question']) > 25 else data['question']
        
        print(f"{run_id:<4} {question:<30} {scores['faithfulness']:<12.3f} {scores['answer_relevancy']:<15.3f} {scores['context_precision']:<16.3f} {scores['context_recall']:<13.3f}")
    
    # 5. æ˜¾ç¤ºæ±‡æ€»ç»“æœ
    print(f"\\nğŸ“ˆ æ±‡æ€»ç»“æœ:")
    avg_scores = results['avg_scores']
    total_score = 0
    valid_metrics = 0
    
    for metric, score in avg_scores.items():
        status = "âœ…" if score > 0 else "âš ï¸"
        print(f"   {status} {metric}: {score:.4f}")
        if score > 0:
            total_score += score
            valid_metrics += 1
    
    if valid_metrics > 0:
        overall_avg = total_score / valid_metrics
        print(f"\\nğŸ¯ æ€»ä½“å¹³å‡åˆ†: {overall_avg:.4f}")
        print(f"ğŸ“Š æœ‰æ•ˆæŒ‡æ ‡: {valid_metrics}/4 ({(valid_metrics/4)*100:.1f}%)")
        
        if valid_metrics == 4:
            print(f"\\nğŸ‰ è¯„æµ‹æˆåŠŸï¼æ‰€æœ‰æŒ‡æ ‡éƒ½æ­£å¸¸å·¥ä½œ")
        elif valid_metrics >= 3:
            print(f"\\nâœ… è¯„æµ‹åŸºæœ¬æˆåŠŸï¼Œå¤§éƒ¨åˆ†æŒ‡æ ‡æ­£å¸¸")
        else:
            print(f"\\nâš ï¸  è¯„æµ‹éƒ¨åˆ†æˆåŠŸï¼Œä»éœ€ä¼˜åŒ–")
    else:
        print(f"\\nâŒ è¯„æµ‹å¤±è´¥ï¼Œæ‰€æœ‰æŒ‡æ ‡éƒ½ä¸º0")
    
    # 6. ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    result_file = f"real_data_evaluation_results_{timestamp}.json"
    
    # æ·»åŠ åŸå§‹æ•°æ®åˆ°ç»“æœä¸­
    full_results = {
        'evaluation_results': results,
        'original_data': real_data,
        'metadata': {
            'timestamp': timestamp,
            'evaluator': 'EnhancedRAGASEvaluator',
            'total_samples': len(real_data),
            'valid_metrics': valid_metrics,
            'overall_average': overall_avg if valid_metrics > 0 else 0.0
        }
    }
    
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(full_results, f, ensure_ascii=False, indent=2)
    
    print(f"\\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    
    return results

if __name__ == "__main__":
    run_evaluation()
    
    print("\\n" + "=" * 70)
    print("ğŸ¯ çœŸå®æ•°æ®è¯„æµ‹å®Œæˆ")
    print("=" * 70)