#!/usr/bin/env python3
"""
æ‰¹é‡ä½¿ç”¨çœŸå®RAG+LLMæ¨ç†APIæ•°æ®è¿›è¡ŒRAGASè¯„æµ‹
"""

import os
import sys
import json
import requests
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(f'real_rag_batch_evaluation_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    ]
)
logger = logging.getLogger(__name__)

# APIé…ç½®
API_BASE = "http://127.0.0.1:8000/api/v1"
RAG_API_URL = f"{API_BASE}/acrac/rag-llm/intelligent-recommendation"

def load_environment():
    """åŠ è½½ç¯å¢ƒå˜é‡"""
    env_file = project_root / ".env"
    if env_file.exists():
        with open(env_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key] = value
        logger.info("âœ… ç¯å¢ƒå˜é‡åŠ è½½å®Œæˆ")
    else:
        logger.warning("âš ï¸ .envæ–‡ä»¶ä¸å­˜åœ¨")

def call_real_rag_api(clinical_query: str, ground_truth: str = "") -> Dict[str, Any]:
    """è°ƒç”¨çœŸå®çš„RAG+LLMæ¨ç†API"""
    try:
        payload = {
            "clinical_query": clinical_query,
            "include_raw_data": True,
            "debug_mode": True,
            "top_scenarios": 5,
            "top_recommendations_per_scenario": 3,
            "show_reasoning": True,
            "similarity_threshold": 0.6,
            "compute_ragas": False,
            "ground_truth": ground_truth
        }
        
        logger.info(f"è°ƒç”¨RAG API: {clinical_query[:50]}...")
        response = requests.post(RAG_API_URL, json=payload, timeout=120)
        response.raise_for_status()
        
        result = response.json()
        if result.get("success"):
            logger.info("âœ… RAG APIè°ƒç”¨æˆåŠŸ")
            return result
        else:
            logger.error(f"âŒ RAG APIè°ƒç”¨å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
            return {}
            
    except Exception as e:
        logger.error(f"âŒ RAG APIè°ƒç”¨å¼‚å¸¸: {e}")
        return {}

def extract_contexts_from_trace(trace_data: Dict[str, Any]) -> List[str]:
    """ä»traceæ•°æ®ä¸­æå–ä¸Šä¸‹æ–‡"""
    contexts = []
    
    # ä»scenariosä¸­æå–
    scenarios = trace_data.get("scenarios", [])
    for scenario in scenarios:
        if isinstance(scenario, dict):
            desc = scenario.get("description_zh", "")
            if desc:
                contexts.append(desc)
    
    # ä»scenarios_with_recommendationsä¸­æå–
    scenarios_with_recs = trace_data.get("scenarios_with_recommendations", [])
    for scenario in scenarios_with_recs:
        if isinstance(scenario, dict):
            desc = scenario.get("description_zh", "")
            if desc and desc not in contexts:
                contexts.append(desc)
    
    return contexts

def convert_to_ragas_format(rag_result: Dict[str, Any], clinical_query: str, ground_truth: str) -> Dict[str, Any]:
    """å°†RAG APIç»“æœè½¬æ¢ä¸ºRAGASè¯„æµ‹æ ¼å¼"""
    
    # æå–LLMæ¨èç»“æœä½œä¸ºç­”æ¡ˆ
    llm_recommendations = rag_result.get("llm_recommendations", {})
    recommendations = llm_recommendations.get("recommendations", [])
    
    # æ„é€ ç­”æ¡ˆæ–‡æœ¬
    answer_parts = []
    if llm_recommendations.get("summary"):
        answer_parts.append(f"æ¨èæ€»ç»“: {llm_recommendations['summary']}")
    
    for i, rec in enumerate(recommendations[:3], 1):
        rec_text = f"{i}. {rec.get('procedure_name', 'æœªçŸ¥æ£€æŸ¥')} (è¯„åˆ†: {rec.get('appropriateness_rating', 'N/A')})"
        if rec.get('recommendation_reason'):
            rec_text += f" - {rec['recommendation_reason']}"
        answer_parts.append(rec_text)
    
    answer = "\n".join(answer_parts) if answer_parts else json.dumps(llm_recommendations, ensure_ascii=False)
    
    # æå–ä¸Šä¸‹æ–‡
    contexts = []
    
    # ä»traceä¸­æå–
    trace_data = rag_result.get("trace", {})
    if trace_data:
        contexts.extend(extract_contexts_from_trace(trace_data))
    
    # ä»scenariosä¸­æå–
    scenarios = rag_result.get("scenarios", [])
    for scenario in scenarios:
        if isinstance(scenario, dict):
            desc = scenario.get("description_zh", "")
            if desc and desc not in contexts:
                contexts.append(desc)
    
    # å¦‚æœæ²¡æœ‰ä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨é»˜è®¤å€¼
    if not contexts:
        contexts = ["æ— å¯ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯"]
    
    return {
        "question": clinical_query,
        "answer": answer,
        "contexts": contexts,
        "ground_truth": ground_truth
    }

def run_ragas_evaluation(ragas_data_list: List[Dict[str, Any]]) -> Dict[str, Any]:
    """è¿è¡Œæ‰¹é‡RAGASè¯„æµ‹"""
    try:
        # ä½¿ç”¨é¡¹ç›®ä¸­å·²æœ‰çš„RAGASè¯„ä¼°å™¨
        from app.services.ragas_evaluator_v2 import ACRACRAGASEvaluator
        
        logger.info("åˆå§‹åŒ–RAGASè¯„ä¼°å™¨...")
        evaluator = ACRACRAGASEvaluator()
        
        # åˆ›å»ºæµ‹è¯•æ ·æœ¬åˆ—è¡¨
        test_samples = []
        for ragas_data in ragas_data_list:
            test_sample = {
                "question": ragas_data["question"],
                "answer": ragas_data["answer"],
                "contexts": ragas_data["contexts"],
                "ground_truth": ragas_data["ground_truth"]
            }
            test_samples.append(test_sample)
        
        # è¿è¡Œæ‰¹é‡è¯„æµ‹
        logger.info(f"å¼€å§‹æ‰¹é‡RAGASè¯„æµ‹ï¼Œå…±{len(test_samples)}ä¸ªæ ·æœ¬...")
        results = evaluator.evaluate_batch(test_samples)
        
        logger.info("âœ… æ‰¹é‡RAGASè¯„æµ‹å®Œæˆ")
        return results
        
    except ImportError as e:
        logger.warning(f"âš ï¸ RAGASåº“æœªå®‰è£…æˆ–é…ç½®ä¸æ­£ç¡®: {e}")
        return {
            "avg_scores": {
                "faithfulness": 0.0,
                "answer_relevancy": 0.0,
                "context_precision": 0.0,
                "context_recall": 0.0
            },
            "individual_scores": [],
            "total_samples": 0,
            "overall_score": 0.0
        }
    except Exception as e:
        logger.error(f"âŒ æ‰¹é‡RAGASè¯„æµ‹å¤±è´¥: {e}")
        return {
            "avg_scores": {
                "faithfulness": 0.0,
                "answer_relevancy": 0.0,
                "context_precision": 0.0,
                "context_recall": 0.0
            },
            "individual_scores": [],
            "total_samples": 0,
            "overall_score": 0.0
        }

def save_evaluation_result(result: Dict[str, Any], filename: str = None):
    """ä¿å­˜è¯„æµ‹ç»“æœ"""
    if not filename:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"real_rag_batch_evaluation_result_{timestamp}.json"
    
    output_path = project_root / "uploads" / "ragas" / filename
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    logger.info(f"âœ… è¯„æµ‹ç»“æœå·²ä¿å­˜: {output_path}")
    return output_path

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹æ‰¹é‡çœŸå®RAG+LLMæ•°æ®çš„RAGASè¯„æµ‹")
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_environment()
    
    # å®šä¹‰å¤šä¸ªæµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            "clinical_query": "50å²å¥³æ€§ï¼Œ3å¤©å‰å¼€å§‹å‡ºç°å·¦ä¾§è‚¢ä½“æ— åŠ›ï¼Œä¼´æ–°å‘å¤´ç—›ã€‚",
            "ground_truth": "MRé¢…è„‘(å¹³æ‰«+å¢å¼º)"
        },
        {
            "clinical_query": "35å²ç”·æ€§ï¼Œæ€¥æ€§èƒ¸ç—›ï¼Œç–‘ä¼¼å¿ƒè‚Œæ¢—æ­»ã€‚",
            "ground_truth": "CTå† çŠ¶åŠ¨è„‰æˆåƒ"
        },
        {
            "clinical_query": "60å²å¥³æ€§ï¼Œæ…¢æ€§å’³å—½3ä¸ªæœˆï¼Œä¼´ä½“é‡ä¸‹é™ã€‚",
            "ground_truth": "CTèƒ¸éƒ¨(å¹³æ‰«+å¢å¼º)"
        },
        {
            "clinical_query": "25å²å¥³æ€§ï¼Œæ€¥æ€§è…¹ç—›ï¼Œå³ä¸‹è…¹å‹ç—›æ˜æ˜¾ã€‚",
            "ground_truth": "CTè…¹éƒ¨(å¹³æ‰«)"
        },
        {
            "clinical_query": "45å²ç”·æ€§ï¼Œåå¤å¤´æ™•ï¼Œä¼´å¬åŠ›ä¸‹é™ã€‚",
            "ground_truth": "MRå†…è€³(å¹³æ‰«+å¢å¼º)"
        }
    ]
    
    logger.info(f"æµ‹è¯•ç”¨ä¾‹æ•°é‡: {len(test_cases)}")
    
    # æ‰¹é‡å¤„ç†æµ‹è¯•ç”¨ä¾‹
    all_rag_results = []
    all_ragas_data = []
    
    for i, test_case in enumerate(test_cases, 1):
        logger.info(f"å¤„ç†ç¬¬{i}/{len(test_cases)}ä¸ªæµ‹è¯•ç”¨ä¾‹...")
        logger.info(f"ä¸´åºŠæŸ¥è¯¢: {test_case['clinical_query']}")
        logger.info(f"æ ‡å‡†ç­”æ¡ˆ: {test_case['ground_truth']}")
        
        # 1. è°ƒç”¨çœŸå®RAG API
        rag_result = call_real_rag_api(
            clinical_query=test_case["clinical_query"],
            ground_truth=test_case["ground_truth"]
        )
        
        if not rag_result:
            logger.warning(f"âš ï¸ ç¬¬{i}ä¸ªæµ‹è¯•ç”¨ä¾‹RAG APIè°ƒç”¨å¤±è´¥ï¼Œè·³è¿‡")
            continue
        
        # 2. è½¬æ¢ä¸ºRAGASæ ¼å¼
        ragas_data = convert_to_ragas_format(
            rag_result=rag_result,
            clinical_query=test_case["clinical_query"],
            ground_truth=test_case["ground_truth"]
        )
        
        all_rag_results.append({
            "test_case": test_case,
            "rag_result": rag_result,
            "ragas_data": ragas_data
        })
        all_ragas_data.append(ragas_data)
        
        logger.info(f"âœ… ç¬¬{i}ä¸ªæµ‹è¯•ç”¨ä¾‹æ•°æ®å‡†å¤‡å®Œæˆ")
    
    if not all_ragas_data:
        logger.error("âŒ æ²¡æœ‰æˆåŠŸå¤„ç†çš„æµ‹è¯•ç”¨ä¾‹ï¼Œé€€å‡º")
        return
    
    # 3. è¿è¡Œæ‰¹é‡RAGASè¯„æµ‹
    logger.info("ğŸ“Š è¿è¡Œæ‰¹é‡RAGASè¯„æµ‹...")
    ragas_results = run_ragas_evaluation(all_ragas_data)
    
    # 4. æ„é€ å®Œæ•´ç»“æœ
    evaluation_result = {
        "timestamp": datetime.now().isoformat(),
        "test_cases_count": len(test_cases),
        "successful_cases_count": len(all_rag_results),
        "test_cases": all_rag_results,
        "ragas_evaluation": ragas_results,
        "summary": {
            "avg_scores": ragas_results.get("avg_scores", {}),
            "overall_score": ragas_results.get("overall_score", 0.0),
            "total_samples": ragas_results.get("total_samples", 0),
            "best_performing_metric": None,
            "worst_performing_metric": None
        }
    }
    
    # åˆ†ææœ€ä½³å’Œæœ€å·®æŒ‡æ ‡
    avg_scores = ragas_results.get("avg_scores", {})
    if avg_scores:
        best_metric = max(avg_scores.items(), key=lambda x: x[1])
        worst_metric = min(avg_scores.items(), key=lambda x: x[1])
        evaluation_result["summary"]["best_performing_metric"] = best_metric
        evaluation_result["summary"]["worst_performing_metric"] = worst_metric
    
    # 5. ä¿å­˜ç»“æœ
    output_path = save_evaluation_result(evaluation_result)
    
    # 6. è¾“å‡ºæ€»ç»“
    logger.info("ğŸ“‹ æ‰¹é‡è¯„æµ‹ç»“æœæ€»ç»“:")
    logger.info(f"  æµ‹è¯•ç”¨ä¾‹æ€»æ•°: {len(test_cases)}")
    logger.info(f"  æˆåŠŸå¤„ç†æ•°: {len(all_rag_results)}")
    logger.info(f"  å¹³å‡è¯„åˆ†:")
    for metric, score in avg_scores.items():
        logger.info(f"    {metric}: {score:.3f}")
    logger.info(f"  ç»¼åˆè¯„åˆ†: {ragas_results.get('overall_score', 0):.3f}")
    
    if evaluation_result["summary"]["best_performing_metric"]:
        best_name, best_score = evaluation_result["summary"]["best_performing_metric"]
        logger.info(f"  æœ€ä½³æŒ‡æ ‡: {best_name} ({best_score:.3f})")
    
    if evaluation_result["summary"]["worst_performing_metric"]:
        worst_name, worst_score = evaluation_result["summary"]["worst_performing_metric"]
        logger.info(f"  æœ€å·®æŒ‡æ ‡: {worst_name} ({worst_score:.3f})")
    
    logger.info("ğŸ‰ æ‰¹é‡çœŸå®RAG+LLMæ•°æ®çš„RAGASè¯„æµ‹å®Œæˆ!")
    return output_path

if __name__ == "__main__":
    main()