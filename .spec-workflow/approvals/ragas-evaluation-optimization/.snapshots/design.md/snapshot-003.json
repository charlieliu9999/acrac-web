{
  "id": "snapshot_1758726978023_c4iosr99y",
  "approvalId": "approval_1758726945539_1v7i535g4",
  "approvalTitle": "RAGAS 评测功能完成和优化 - 修订版技术设计文档审批",
  "version": 3,
  "timestamp": "2025-09-24T15:16:18.023Z",
  "trigger": "approved",
  "status": "pending",
  "content": "# RAGAS 评测功能完成和优化技术设计文档\n\n## 概述\n\n本设计文档基于已批准的需求文档，详细描述了 RAGAS 评测功能完成和优化的技术架构、组件设计和实现方案。系统采用模块化设计，支持通用评测任务，能够灵活配置不同的评测模型和流程，提供分步骤的前端评测体验。\n\n## 技术标准对齐\n\n### 技术标准 (tech.md)\n- 遵循 FastAPI 后端架构模式，使用 Pydantic 进行数据验证\n- 采用 React + TypeScript 前端架构，使用 Ant Design 组件库\n- 遵循 RESTful API 设计原则，提供统一的 API 接口\n- 使用 SQLAlchemy ORM 进行数据库操作，支持 PostgreSQL 数据库\n- 采用异步编程模式，支持高并发处理\n\n### 项目结构 (structure.md)\n- 后端服务位于 `backend/app/` 目录，按功能模块组织\n- 前端页面位于 `frontend/src/pages/` 目录\n- API 端点位于 `backend/app/api/api_v1/endpoints/` 目录\n- 数据模型位于 `backend/app/models/` 目录\n- 服务层位于 `backend/app/services/` 目录\n\n## 代码重用分析\n\n### 现有组件利用\n- **增强版 RAGAS 评估器**: 利用现有的 `enhanced_ragas_evaluator.py` 作为核心评估引擎，该评估器专门处理中文医学内容，具有更好的 LLM 输出解析能力\n- **RAGAS 评估器 V2**: 作为备用评估器，利用现有的 `ragas_evaluator_v2.py`\n- **前端评测界面**: 基于现有的 `RAGASEvalV2.tsx` 进行优化和扩展\n- **API 路由系统**: 利用现有的 FastAPI 路由架构\n- **数据库模型**: 利用现有的 `inference_logs` 表结构\n- **配置管理**: 利用现有的配置管理系统\n\n### 集成点\n- **RAG 推理服务**: 集成现有的 `/api/v1/acrac/rag-llm/intelligent-recommendation` API\n- **推理记录服务**: 集成现有的 `/api/v1/acrac/rag-llm/runs/log` API\n- **数据库存储**: 利用现有的 PostgreSQL 数据库和表结构\n- **文件存储**: 利用现有的文件上传和存储机制\n\n## 架构设计\n\n### 整体架构\n\n```mermaid\ngraph TB\n    subgraph \"前端层 (React + TypeScript)\"\n        UI[分步骤评测界面]\n        Upload[数据上传组件]\n        Preview[数据预览组件]\n        Selection[样本选择组件]\n        Inference[推理进度组件]\n        Eval[评测进度组件]\n        Results[结果可视化组件]\n    end\n    \n    subgraph \"API层 (FastAPI)\"\n        Gateway[API网关]\n        Auth[认证中间件]\n        RateLimit[限流中间件]\n        TaskManager[任务管理器]\n    end\n    \n    subgraph \"业务逻辑层\"\n        EvalEngine[增强版模块化评测引擎]\n        DataProcessor[增强版数据转换器]\n        ConfigManager[配置管理器]\n        ModelManager[模型管理器]\n        ProcessTracker[过程跟踪器]\n        ResultAnalyzer[评测结果分析器]\n    end\n    \n    subgraph \"数据层\"\n        FileStorage[文件存储]\n        Cache[缓存层]\n        Database[数据库]\n        ProcessData[过程数据存储]\n    end\n    \n    subgraph \"外部服务\"\n        LLM[可配置LLM服务]\n        Embedding[可配置Embedding服务]\n        Rerank[可配置Rerank服务]\n        RAG[RAG推理服务]\n    end\n    \n    UI --> Gateway\n    Gateway --> EvalEngine\n    EvalEngine --> DataProcessor\n    DataProcessor --> LLM\n    EvalEngine --> RAG\n    ModelManager --> LLM\n    ModelManager --> Embedding\n    ModelManager --> Rerank\n    ProcessTracker --> ProcessData\n    ConfigManager --> Database\n    TaskManager --> Cache\n```\n\n### 模块化设计原则\n- **单一文件职责**: 每个文件处理一个特定的关注点或领域\n- **组件隔离**: 创建小型、专注的组件而非大型单体文件\n- **服务层分离**: 分离数据访问、业务逻辑和表现层\n- **工具模块化**: 将工具分解为专注的、单一用途的模块\n- **插件化架构**: 支持不同评测指标和模型的插件式集成\n\n## 组件和接口设计\n\n### 1. 增强版模块化评测引擎 (EnhancedModularEvaluationEngine)\n\n**目的**: 提供可配置、可扩展的 RAGAS 评测引擎，专门优化中文医学内容评测，支持通用评测任务\n\n**接口**:\n```python\nclass EnhancedModularEvaluationEngine:\n    async def initialize_models(self, config: ModelConfig) -> None\n    async def evaluate_sample(self, sample: EvaluationSample) -> EvaluationResult\n    async def evaluate_batch(self, samples: List[EvaluationSample]) -> BatchEvaluationResult\n    async def get_available_metrics(self) -> List[MetricInfo]\n    async def update_config(self, config: EvaluationConfig) -> None\n    async def _evaluate_faithfulness_enhanced(self, sample: SingleTurnSample) -> float\n    async def _evaluate_context_precision_enhanced(self, sample: SingleTurnSample) -> float\n    async def _evaluate_context_recall_enhanced(self, sample: SingleTurnSample) -> float\n    async def _evaluate_answer_relevancy_enhanced(self, sample: SingleTurnSample) -> float\n```\n\n**核心特性**:\n- **增强版中文支持**: 专门优化的中文医学内容评测逻辑\n- **改进的 LLM 输出解析**: 支持 JSON 格式解析和启发式回退机制\n- **医学领域优化**: 针对医学术语和临床指南的评测优化\n- **错误处理增强**: 完善的异常处理和降级机制\n\n**依赖**: \n- RAGAS 0.3.x 库\n- 可配置的 LLM 和 Embedding 模型\n- 数据转换器\n- 中文医学知识库\n\n**重用**: 基于现有的 `enhanced_ragas_evaluator.py` 进行重构和扩展\n\n### 2. 分步骤前端评测流程 (StepBasedEvaluationFlow)\n\n**目的**: 提供分步骤的评测界面，包括数据上传、解析、勾选、推理、评测和结果展示\n\n**接口**:\n```typescript\ninterface StepBasedEvaluationFlow {\n  // 步骤1: 数据上传和解析\n  uploadAndParseData(file: File): Promise<ParseResult>\n  \n  // 步骤2: 样本选择\n  selectSamples(samples: TestCase[]): Promise<SelectedSamples>\n  \n  // 步骤3: 推理执行\n  executeInference(samples: SelectedSamples): Promise<InferenceResult>\n  \n  // 步骤4: 推理结果可视化\n  visualizeInferenceData(result: InferenceResult): Promise<void>\n  \n  // 步骤5: 评测样本选择\n  selectEvaluationSamples(inferenceResult: InferenceResult): Promise<SelectedEvalSamples>\n  \n  // 步骤6: 评测执行\n  executeEvaluation(samples: SelectedEvalSamples): Promise<EvaluationResult>\n  \n  // 步骤7: 结果可视化\n  visualizeEvaluationResult(result: EvaluationResult): Promise<void>\n}\n```\n\n**依赖**: \n- Ant Design 组件库\n- 文件上传组件\n- 数据表格组件\n- 进度条组件\n\n**重用**: 基于现有的 `RAGASEvalV2.tsx` 进行重构\n\n### 3. 可配置模型管理器 (ConfigurableModelManager)\n\n**目的**: 管理可配置的 LLM、Embedding 和 Rerank 模型\n\n**接口**:\n```python\nclass ConfigurableModelManager:\n    async def load_llm_model(self, config: LLMConfig) -> LLMModel\n    async def load_embedding_model(self, config: EmbeddingConfig) -> EmbeddingModel\n    async def load_rerank_model(self, config: RerankConfig) -> RerankModel\n    async def get_available_models(self) -> List[ModelInfo]\n    async def validate_model_config(self, config: ModelConfig) -> ValidationResult\n```\n\n**依赖**: \n- LangChain 模型接口\n- 配置管理系统\n- 模型注册表\n\n**重用**: 利用现有的配置管理系统\n\n### 4. 过程数据跟踪器 (ProcessDataTracker)\n\n**目的**: 跟踪和存储推理和评测的详细过程数据\n\n**接口**:\n```python\nclass ProcessDataTracker:\n    async def track_inference_process(self, task_id: str, data: InferenceProcessData) -> None\n    async def track_evaluation_process(self, task_id: str, data: EvaluationProcessData) -> None\n    async def get_process_data(self, task_id: str) -> ProcessData\n    async def visualize_process_data(self, task_id: str) -> VisualizationData\n```\n\n**依赖**: \n- 数据库存储\n- 缓存系统\n- 数据序列化\n\n**重用**: 利用现有的数据库和缓存系统\n\n### 5. 增强版数据转换器 (EnhancedDataConverter)\n\n**目的**: 将不同格式的数据转换为增强版 RAGAS 评测格式，专门优化中文医学内容处理\n\n**接口**:\n```python\nclass EnhancedDataConverter:\n    async def convert_rag_result(self, rag_result: RAGResult) -> RAGASSample\n    async def convert_excel_data(self, excel_data: ExcelData) -> List[RAGASSample]\n    async def validate_sample(self, sample: RAGASSample) -> ValidationResult\n    async def normalize_contexts(self, contexts: List[str]) -> List[str]\n    async def create_enhanced_sample(self, data: Dict[str, Any]) -> SingleTurnSample\n    async def preprocess_chinese_content(self, content: str) -> str\n    async def extract_medical_terms(self, text: str) -> List[str]\n```\n\n**核心特性**:\n- **中文医学内容预处理**: 专门处理中文医学术语和概念\n- **上下文智能过滤**: 过滤空上下文并提供默认值\n- **医学术语提取**: 自动识别和提取医学专业术语\n- **数据质量验证**: 增强的数据验证和错误处理\n\n**依赖**: \n- 数据验证库\n- 中文文本处理工具\n- 医学术语库\n- 格式转换器\n\n**重用**: 基于现有的数据转换逻辑和 `enhanced_ragas_evaluator.py` 中的 `create_sample` 方法\n\n### 6. 评测结果分析器 (EvaluationResultAnalyzer)\n\n**目的**: 分析评测结果，提供详细的诊断和改进建议\n\n**接口**:\n```python\nclass EvaluationResultAnalyzer:\n    async def analyze_scores(self, results: List[EvaluationResult]) -> AnalysisReport\n    async def identify_issues(self, result: EvaluationResult) -> List[Issue]\n    async def generate_improvement_suggestions(self, issues: List[Issue]) -> List[Suggestion]\n    async def compare_with_baseline(self, results: List[EvaluationResult]) -> ComparisonReport\n    async def export_analysis_report(self, analysis: AnalysisReport) -> str\n```\n\n**核心特性**:\n- **智能问题诊断**: 自动识别评测中的问题\n- **改进建议生成**: 基于评测结果提供具体的改进建议\n- **基准对比**: 与历史数据或标准基准进行对比\n- **可视化报告**: 生成易于理解的评测分析报告\n\n**依赖**: \n- 数据分析库\n- 可视化工具\n- 报告生成器\n\n**重用**: 基于现有的评测结果处理逻辑\n\n## 数据模型\n\n### 增强版评测样本模型 (EnhancedEvaluationSample)\n```\nEnhancedEvaluationSample:\n- id: str (唯一标识符)\n- question: str (问题文本)\n- answer: str (生成的答案)\n- contexts: List[str] (检索的上下文)\n- ground_truth: Optional[str] (标准答案)\n- medical_terms: List[str] (提取的医学术语)\n- chinese_processed: bool (是否经过中文预处理)\n- metadata: Dict[str, Any] (元数据)\n- created_at: datetime (创建时间)\n```\n\n### 评测样本模型 (EvaluationSample)\n```\nEvaluationSample:\n- id: str (唯一标识符)\n- question: str (问题文本)\n- answer: str (生成的答案)\n- contexts: List[str] (检索的上下文)\n- ground_truth: Optional[str] (标准答案)\n- metadata: Dict[str, Any] (元数据)\n- created_at: datetime (创建时间)\n```\n\n### 增强版评测结果模型 (EnhancedEvaluationResult)\n```\nEnhancedEvaluationResult:\n- sample_id: str (样本ID)\n- metrics: Dict[str, float] (评测指标分数)\n- enhanced_metrics: Dict[str, float] (增强版评测指标分数)\n- llm_parsing_success: bool (LLM输出解析是否成功)\n- fallback_method_used: str (使用的回退方法)\n- medical_term_analysis: Dict[str, Any] (医学术语分析结果)\n- chinese_processing_info: Dict[str, Any] (中文处理信息)\n- process_data: Dict[str, Any] (评测过程数据)\n- status: str (评测状态)\n- error_message: Optional[str] (错误信息)\n- created_at: datetime (创建时间)\n```\n\n### 评测结果模型 (EvaluationResult)\n```\nEvaluationResult:\n- sample_id: str (样本ID)\n- metrics: Dict[str, float] (评测指标分数)\n- process_data: Dict[str, Any] (评测过程数据)\n- status: str (评测状态)\n- error_message: Optional[str] (错误信息)\n- created_at: datetime (创建时间)\n```\n\n### 任务模型 (EvaluationTask)\n```\nEvaluationTask:\n- task_id: str (任务唯一标识符)\n- status: str (任务状态)\n- config: EvaluationConfig (评测配置)\n- samples: List[EvaluationSample] (评测样本)\n- results: List[EvaluationResult] (评测结果)\n- process_data: ProcessData (过程数据)\n- created_at: datetime (创建时间)\n- completed_at: Optional[datetime] (完成时间)\n```\n\n### 模型配置模型 (ModelConfig)\n```\nModelConfig:\n- llm_config: LLMConfig (LLM模型配置)\n- embedding_config: EmbeddingConfig (嵌入模型配置)\n- rerank_config: Optional[RerankConfig] (重排序模型配置)\n- evaluation_config: EvaluationConfig (评测配置)\n- created_at: datetime (创建时间)\n- updated_at: datetime (更新时间)\n```\n\n## 错误处理\n\n### 错误场景\n\n1. **模型加载失败**\n   - **处理**: 提供降级方案，使用默认模型或返回错误信息\n   - **用户影响**: 显示友好的错误提示，提供重试选项\n\n2. **评测过程异常**\n   - **处理**: 记录详细错误日志，提供部分结果\n   - **用户影响**: 显示错误状态，提供重新评测选项\n\n3. **数据格式错误**\n   - **处理**: 验证数据格式，提供修复建议\n   - **用户影响**: 显示数据验证错误，提供数据修复指导\n\n4. **网络连接问题**\n   - **处理**: 实现重试机制和超时处理\n   - **用户影响**: 显示连接状态，提供手动重试选项\n\n5. **存储空间不足**\n   - **处理**: 清理临时文件，提供存储管理建议\n   - **用户影响**: 显示存储警告，提供清理选项\n\n## 测试策略\n\n### 单元测试\n- **评测引擎测试**: 测试各个评测指标的计算逻辑\n- **数据转换测试**: 测试不同格式数据的转换功能\n- **模型管理测试**: 测试模型加载和配置功能\n- **错误处理测试**: 测试各种异常情况的处理\n\n### 集成测试\n- **API 集成测试**: 测试前后端 API 的集成\n- **数据库集成测试**: 测试数据存储和检索功能\n- **外部服务集成测试**: 测试与 LLM 和 RAG 服务的集成\n\n### 端到端测试\n- **完整评测流程测试**: 测试从数据上传到结果展示的完整流程\n- **多用户并发测试**: 测试多用户同时进行评测的场景\n- **性能压力测试**: 测试大量数据评测的性能表现\n\n## 部署和配置\n\n### 环境配置\n- **开发环境**: 使用本地模型和数据库\n- **测试环境**: 使用测试数据和模拟服务\n- **生产环境**: 使用生产级模型和数据库\n\n### 配置管理\n- **模型配置**: 支持动态配置不同的 LLM 和 Embedding 模型\n- **评测配置**: 支持配置评测参数和阈值\n- **系统配置**: 支持配置并发数、超时时间等系统参数\n\n### 监控和日志\n- **性能监控**: 监控评测性能和资源使用情况\n- **错误监控**: 监控系统错误和异常情况\n- **用户行为监控**: 监控用户使用情况和反馈\n\n## 安全考虑\n\n### 数据安全\n- **数据加密**: 对敏感数据进行加密存储\n- **访问控制**: 实现基于角色的访问控制\n- **数据脱敏**: 对评测数据进行脱敏处理\n\n### 系统安全\n- **输入验证**: 严格验证所有输入数据\n- **API 安全**: 实现 API 认证和授权\n- **日志安全**: 确保日志不包含敏感信息\n",
  "fileStats": {
    "size": 15136,
    "lines": 424,
    "lastModified": "2025-09-24T15:15:57.942Z"
  },
  "comments": []
}